{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\n================================================================================\nLEARNED ATTENTION DISTILLATION: TWO NOVEL APPROACHES\n================================================================================\nHardware: Kaggle P100 GPU (16GB VRAM)\n\nNOVEL CONTRIBUTIONS:\n\nModel A (Teacher): Standard ViT with O(n²) Self-Attention\n\nModel B (Student 1): Multi-Scale Kernel Attention (MSKA)\n  - Multiple parallel kernel branches at different ranks (16, 64, 128)\n  - Dynamic scale mixing: input-dependent blending of scales\n  - Novel: Learns WHICH scale matters per input via distillation\n  - Complexity: O(n × r²) where r = max kernel rank\n\nModel C (Student 2): Learned Nyström Attention (LNA)\n  - Approximates full attention via learned landmark tokens\n  - Novel: Landmarks are LEARNED via distillation (not random sampling)\n  - Complexity: O(n × m²) where m = num_landmarks << n\n\nTraining Pipeline:\n  Phase 1: Train Teacher\n  Phase 2: Distill to Student 1 (MSKA)\n  Phase 3: Distill to Student 2 (LNA)\n  Phase 4: Fine-tune Students for classification\n\nExperiments:\n  - CIFAR-10 & CIFAR-100 (50% data for speed)\n  - 3 random seeds\n  - Statistical significance tests\n  - Attention fidelity analysis\n  - Theoretical complexity comparison\n  - Ablation studies\n================================================================================\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport math\nimport random\nimport warnings\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom scipy.stats import pearsonr, ttest_rel, sem\nimport torchvision\nimport torchvision.transforms as transforms\n\nwarnings.filterwarnings('ignore')\n\n\n# ================================================================================\n# SECTION 1: CONFIGURATION\n# ================================================================================\n\n@dataclass\nclass Config:\n    \"\"\"Central configuration for all experiments.\"\"\"\n    DEVICE: torch.device = None\n    SEEDS: List[int] = field(default_factory=lambda: [42, 123, 456])\n    \n    # Architecture\n    DIM: int = 192\n    DEPTH: int = 6\n    HEADS: int = 6\n    MLP_RATIO: float = 2.0\n    DROPOUT: float = 0.1\n    PATCH_SIZE: int = 4\n    \n    # MSKA-specific hyperparameters\n    MSKA_RANKS: List[int] = field(default_factory=lambda: [16, 64, 128])\n    \n    # LNA-specific hyperparameters\n    NUM_LANDMARKS: int = 16\n    \n    # Training configuration\n    BATCH_SIZE: int = 256\n    EPOCHS_TEACHER: int = 10\n    EPOCHS_DISTILL: int = 10\n    EPOCHS_STUDENT: int = 10\n    LR: float = 1e-3\n    LR_DISTILL: float = 5e-4\n    WD: float = 0.05\n    WARMUP_EPOCHS: int = 3\n    DISTILL_LAMBDA: float = 0.5\n    \n    # Data subsampling (50% for speed)\n    DATA_FRACTION: float = 0.5\n    \n    # Ablation configurations\n    ABLATION_MSKA_SCALES: List[List[int]] = field(default_factory=lambda: [\n        [16, 32],           # Small scales\n        [16, 64, 128],      # Default\n        [32, 64, 128, 256]  # Large scales\n    ])\n    ABLATION_NUM_LANDMARKS: List[int] = field(default_factory=lambda: [8, 16, 32])\n    \n    def __post_init__(self):\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef count_params(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef fmt_params(n: int) -> str:\n    return f\"{n/1e6:.2f}M\" if n >= 1e6 else f\"{n/1e3:.1f}K\"\n\n\ndef header(title: str, char: str = \"=\", width: int = 88):\n    print(f\"\\n{char * width}\")\n    print(f\"{title.center(width)}\")\n    print(f\"{char * width}\")\n\n\ndef subheader(title: str, char: str = \"-\", width: int = 88):\n    print(f\"\\n{char * width}\")\n    print(f\"  {title}\")\n    print(f\"{char * width}\")\n\n\n# ================================================================================\n# SECTION 2: DATASETS (WITH SUBSAMPLING)\n# ================================================================================\n\ndef get_cifar10(cfg: Config):\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262)),\n        transforms.RandomErasing(p=0.1)\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n    ])\n    \n    train_ds = torchvision.datasets.CIFAR10('./data', True, download=True, transform=transform_train)\n    test_ds = torchvision.datasets.CIFAR10('./data', False, download=True, transform=transform_test)\n    \n    # Subsample for speed\n    if cfg.DATA_FRACTION < 1.0:\n        num_train = int(len(train_ds) * cfg.DATA_FRACTION)\n        num_test = int(len(test_ds) * cfg.DATA_FRACTION)\n        train_indices = random.sample(range(len(train_ds)), num_train)\n        test_indices = random.sample(range(len(test_ds)), num_test)\n        train_ds = Subset(train_ds, train_indices)\n        test_ds = Subset(test_ds, test_indices)\n    \n    train_ld = DataLoader(train_ds, cfg.BATCH_SIZE, shuffle=True, num_workers=2, \n                          pin_memory=True, drop_last=True)\n    test_ld = DataLoader(test_ds, cfg.BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    \n    return train_ld, test_ld, 10, 32\n\n\ndef get_cifar100(cfg: Config):\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n        transforms.RandomErasing(p=0.1)\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n    ])\n    \n    train_ds = torchvision.datasets.CIFAR100('./data', True, download=True, transform=transform_train)\n    test_ds = torchvision.datasets.CIFAR100('./data', False, download=True, transform=transform_test)\n    \n    # Subsample for speed\n    if cfg.DATA_FRACTION < 1.0:\n        num_train = int(len(train_ds) * cfg.DATA_FRACTION)\n        num_test = int(len(test_ds) * cfg.DATA_FRACTION)\n        train_indices = random.sample(range(len(train_ds)), num_train)\n        test_indices = random.sample(range(len(test_ds)), num_test)\n        train_ds = Subset(train_ds, train_indices)\n        test_ds = Subset(test_ds, test_indices)\n    \n    train_ld = DataLoader(train_ds, cfg.BATCH_SIZE, shuffle=True, num_workers=2, \n                          pin_memory=True, drop_last=True)\n    test_ld = DataLoader(test_ds, cfg.BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    \n    return train_ld, test_ld, 100, 32\n\n\n# ================================================================================\n# SECTION 3: SHARED COMPONENTS\n# ================================================================================\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int, patch_size: int, in_channels: int, embed_dim: int):\n        super().__init__()\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return self.norm(x)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        return self.dropout(self.fc2(self.dropout(F.gelu(self.fc1(x)))))\n\n\n# ================================================================================\n# SECTION 4: TEACHER - STANDARD VIT (MODEL A)\n# ================================================================================\n\nclass StandardAttention(nn.Module):\n    \"\"\"Standard O(n²) multi-head self-attention.\"\"\"\n    \n    def __init__(self, dim: int, num_heads: int = 6, dropout: float = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(dim, dim * 3)\n        self.proj = nn.Linear(dim, dim)\n        self.attn_dropout = nn.Dropout(dropout)\n        self.proj_dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, return_attn=False):\n        B, N, C = x.shape\n        \n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj_dropout(self.proj(x))\n        \n        if return_attn:\n            return x, attn.detach()\n        return x, None\n\n\nclass StandardTransformerBlock(nn.Module):\n    def __init__(self, dim: int, num_heads: int, mlp_ratio: float, dropout: float = 0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = StandardAttention(dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n    \n    def forward(self, x, return_attn=False):\n        attn_out, attn_map = self.attn(self.norm1(x), return_attn)\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x, attn_map\n\n\nclass StandardViT(nn.Module):\n    \"\"\"Model A: Standard ViT Teacher with O(n²) Attention.\"\"\"\n    \n    def __init__(self, img_size: int, num_classes: int, cfg: Config):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(img_size, cfg.PATCH_SIZE, 3, cfg.DIM)\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.DIM))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, cfg.DIM))\n        self.pos_dropout = nn.Dropout(cfg.DROPOUT)\n        \n        self.blocks = nn.ModuleList([\n            StandardTransformerBlock(cfg.DIM, cfg.HEADS, cfg.MLP_RATIO, cfg.DROPOUT)\n            for _ in range(cfg.DEPTH)\n        ])\n        \n        self.norm = nn.LayerNorm(cfg.DIM)\n        self.head = nn.Linear(cfg.DIM, num_classes)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        self.apply(self._init_module)\n    \n    def _init_module(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n    \n    def get_embeddings(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        return self.pos_dropout(x + self.pos_embed)\n    \n    def forward(self, x, return_attn=False):\n        x = self.get_embeddings(x)\n        \n        attn_maps = []\n        for block in self.blocks:\n            x, attn = block(x, return_attn)\n            if return_attn and attn is not None:\n                attn_maps.append(attn)\n        \n        x = self.norm(x)\n        logits = self.head(x[:, 0])\n        \n        if return_attn:\n            return logits, attn_maps\n        return logits, None\n\n\n# ================================================================================\n# SECTION 5: STUDENT 1 - MULTI-SCALE KERNEL ATTENTION (MSKA)\n# ================================================================================\n\nclass MultiScaleKernelAttention(nn.Module):\n    \"\"\"\n    Multi-Scale Kernel Attention (MSKA) - Option D\n    \n    Novel Contribution:\n    - Multiple parallel kernel branches at different ranks (16, 64, 128)\n    - Each scale captures different attention patterns:\n        * Low rank (16):  Global, coarse patterns\n        * Mid rank (64):  Medium-range dependencies\n        * High rank (128): Fine-grained, detailed patterns\n    - Dynamic scale mixing: input-dependent blending learned via distillation\n    \n    Why This is Novel:\n    - Performer uses SINGLE fixed kernel rank\n    - Linear Transformer uses SINGLE fixed kernel function\n    - We use MULTIPLE scales with LEARNED mixing weights\n    \n    Complexity: O(n × r_max²) where r_max = max(ranks)\n    \"\"\"\n    \n    def __init__(self, dim: int, num_heads: int, ranks: List[int], dropout: float = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.ranks = ranks\n        self.num_scales = len(ranks)\n        \n        # Parallel kernel branches for each scale\n        self.phi_nets = nn.ModuleList()\n        self.psi_nets = nn.ModuleList()\n        \n        for rank in ranks:\n            self.phi_nets.append(nn.Sequential(\n                nn.Linear(dim, rank * num_heads),\n                nn.LayerNorm(rank * num_heads),\n                nn.GELU()\n            ))\n            self.psi_nets.append(nn.Sequential(\n                nn.Linear(dim, rank * num_heads),\n                nn.LayerNorm(rank * num_heads),\n                nn.GELU()\n            ))\n        \n        # Dynamic scale selector (THE NOVEL PART)\n        self.scale_selector = nn.Sequential(\n            nn.Linear(dim, 64),\n            nn.GELU(),\n            nn.Linear(64, self.num_scales),\n            nn.Softmax(dim=-1)\n        )\n        \n        # Value and output projections\n        self.v_proj = nn.Linear(dim, dim)\n        self.out_proj = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, return_attn=False):\n        B, N, C = x.shape\n        \n        # Compute attention at each scale\n        scale_attns = []\n        scale_outputs = []\n        \n        for i, (phi_net, psi_net, rank) in enumerate(zip(self.phi_nets, self.psi_nets, self.ranks)):\n            phi = phi_net(x).view(B, N, self.num_heads, rank).transpose(1, 2)\n            psi = psi_net(x).view(B, N, self.num_heads, rank).transpose(1, 2)\n            \n            # Normalize for stability\n            phi = F.normalize(phi, dim=-1) * math.sqrt(rank)\n            psi = F.normalize(psi, dim=-1)\n            \n            # Compute attention for this scale\n            attn_scale = torch.matmul(phi, psi.transpose(-2, -1))  # [B, H, N, N]\n            attn_scale = F.softmax(attn_scale, dim=-1)\n            scale_attns.append(attn_scale)\n        \n        # Dynamic scale selection based on global context\n        global_ctx = x.mean(dim=1)  # [B, C]\n        scale_weights = self.scale_selector(global_ctx)  # [B, num_scales]\n        \n        # Weighted combination of attention maps\n        combined_attn = torch.zeros(B, self.num_heads, N, N, device=x.device)\n        for i, attn_scale in enumerate(scale_attns):\n            weight = scale_weights[:, i].view(B, 1, 1, 1)\n            combined_attn = combined_attn + weight * attn_scale\n        \n        combined_attn = self.dropout(combined_attn)\n        \n        # Apply attention to values\n        v = self.v_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        out = torch.matmul(combined_attn, v)\n        out = out.transpose(1, 2).reshape(B, N, C)\n        out = self.out_proj(out)\n        \n        if return_attn:\n            return out, combined_attn.detach()\n        return out, None\n    \n    def predict_attention(self, x):\n        \"\"\"Predict attention map for distillation loss.\"\"\"\n        B, N, C = x.shape\n        \n        scale_attns = []\n        for i, (phi_net, psi_net, rank) in enumerate(zip(self.phi_nets, self.psi_nets, self.ranks)):\n            phi = phi_net(x).view(B, N, self.num_heads, rank).transpose(1, 2)\n            psi = psi_net(x).view(B, N, self.num_heads, rank).transpose(1, 2)\n            \n            phi = F.normalize(phi, dim=-1) * math.sqrt(rank)\n            psi = F.normalize(psi, dim=-1)\n            \n            attn_scale = torch.matmul(phi, psi.transpose(-2, -1))\n            attn_scale = F.softmax(attn_scale, dim=-1)\n            scale_attns.append(attn_scale)\n        \n        global_ctx = x.mean(dim=1)\n        scale_weights = self.scale_selector(global_ctx)\n        \n        combined_attn = torch.zeros(B, self.num_heads, N, N, device=x.device)\n        for i, attn_scale in enumerate(scale_attns):\n            weight = scale_weights[:, i].view(B, 1, 1, 1)\n            combined_attn = combined_attn + weight * attn_scale\n        \n        return combined_attn\n    \n    def get_scale_weights(self, x):\n        \"\"\"Get scale weights for analysis.\"\"\"\n        global_ctx = x.mean(dim=1)\n        return self.scale_selector(global_ctx)\n\n\nclass MSKABlock(nn.Module):\n    \"\"\"Transformer block with Multi-Scale Kernel Attention.\"\"\"\n    \n    def __init__(self, dim: int, num_heads: int, ranks: List[int], mlp_ratio: float, dropout: float = 0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = MultiScaleKernelAttention(dim, num_heads, ranks, dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n    \n    def forward(self, x, return_attn=False):\n        attn_out, attn_map = self.attn(self.norm1(x), return_attn)\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x, attn_map\n\n\nclass MSKAViT(nn.Module):\n    \"\"\"Model B: ViT with Multi-Scale Kernel Attention.\"\"\"\n    \n    def __init__(self, img_size: int, num_classes: int, cfg: Config, ranks: List[int] = None):\n        super().__init__()\n        \n        ranks = ranks or cfg.MSKA_RANKS\n        \n        self.patch_embed = PatchEmbedding(img_size, cfg.PATCH_SIZE, 3, cfg.DIM)\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.DIM))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, cfg.DIM))\n        self.pos_dropout = nn.Dropout(cfg.DROPOUT)\n        \n        self.blocks = nn.ModuleList([\n            MSKABlock(cfg.DIM, cfg.HEADS, ranks, cfg.MLP_RATIO, cfg.DROPOUT)\n            for _ in range(cfg.DEPTH)\n        ])\n        \n        self.norm = nn.LayerNorm(cfg.DIM)\n        self.head = nn.Linear(cfg.DIM, num_classes)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        self.apply(self._init_module)\n    \n    def _init_module(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n    \n    def get_embeddings(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        return self.pos_dropout(x + self.pos_embed)\n    \n    def get_all_attention_maps(self, x):\n        \"\"\"Get predicted attention maps for distillation.\"\"\"\n        x = self.get_embeddings(x)\n        attn_maps = []\n        for block in self.blocks:\n            attn = block.attn.predict_attention(block.norm1(x))\n            attn_maps.append(attn)\n            x, _ = block(x, return_attn=False)\n        return attn_maps\n    \n    def get_scale_weights(self, x):\n        \"\"\"Get scale weights from all layers for analysis.\"\"\"\n        x = self.get_embeddings(x)\n        all_weights = []\n        for block in self.blocks:\n            weights = block.attn.get_scale_weights(block.norm1(x))\n            all_weights.append(weights)\n            x, _ = block(x, return_attn=False)\n        return all_weights\n    \n    def forward(self, x, return_attn=False):\n        x = self.get_embeddings(x)\n        \n        attn_maps = []\n        for block in self.blocks:\n            x, attn = block(x, return_attn)\n            if return_attn and attn is not None:\n                attn_maps.append(attn)\n        \n        x = self.norm(x)\n        logits = self.head(x[:, 0])\n        \n        if return_attn:\n            return logits, attn_maps\n        return logits, None\n    \n    def freeze_classifier(self):\n        self.head.requires_grad_(False)\n        self.norm.requires_grad_(False)\n    \n    def unfreeze_all(self):\n        for param in self.parameters():\n            param.requires_grad = True\n\n\n# ================================================================================\n# SECTION 6: STUDENT 2 - LEARNED NYSTRÖM ATTENTION (LNA)\n# ================================================================================\n\nclass LearnedNystromAttention(nn.Module):\n    \"\"\"\n    Learned Nyström Attention (LNA) - Option H\n    \n    Novel Contribution:\n    - Approximates full n×n attention via m << n landmark tokens\n    - Landmarks are LEARNED via distillation (not randomly sampled)\n    - Nyström method: A ≈ A[:,L] @ (A[L,L])^(-1) @ A[L,:]^T\n    \n    Why This is Novel:\n    - Original Nyström uses random landmark sampling\n    - We LEARN which tokens should be landmarks\n    - Landmarks become \"attention hubs\" via distillation\n    \n    Complexity: O(n × m²) where m = num_landmarks << n\n    Memory: O(n × m) instead of O(n²)\n    \"\"\"\n    \n    def __init__(self, dim: int, num_heads: int, num_landmarks: int, dropout: float = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.num_landmarks = num_landmarks\n        self.scale = self.head_dim ** -0.5\n        \n        # Learnable landmark selector (THE NOVEL PART)\n        self.landmark_scorer = nn.Sequential(\n            nn.Linear(dim, 64),\n            nn.GELU(),\n            nn.Linear(64, 1)\n        )\n        \n        # Standard QKV projections\n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n        self.out_proj = nn.Linear(dim, dim)\n        \n        self.attn_dropout = nn.Dropout(dropout)\n        self.proj_dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, return_attn=False):\n        B, N, C = x.shape\n        m = min(self.num_landmarks, N)\n        \n        # Score each token for landmark selection\n        scores = self.landmark_scorer(x).squeeze(-1)  # [B, N]\n        \n        # Soft landmark selection using softmax (differentiable)\n        landmark_weights = F.softmax(scores, dim=-1)  # [B, N]\n        \n        # Get top-m landmarks (for actual computation)\n        topk_vals, topk_idx = torch.topk(scores, m, dim=-1)  # [B, m]\n        \n        # QKV projections\n        q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, N, d_h]\n        k = self.k_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.v_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Gather landmark K and V\n        # Expand indices for gathering\n        topk_idx_expanded = topk_idx.unsqueeze(1).unsqueeze(-1).expand(-1, self.num_heads, -1, self.head_dim)\n        \n        k_landmarks = torch.gather(k, 2, topk_idx_expanded)  # [B, H, m, d_h]\n        v_landmarks = torch.gather(v, 2, topk_idx_expanded)  # [B, H, m, d_h]\n        \n        # Nyström approximation\n        # Step 1: Q to landmarks attention\n        attn_q_to_l = torch.matmul(q, k_landmarks.transpose(-2, -1)) * self.scale  # [B, H, N, m]\n        attn_q_to_l = F.softmax(attn_q_to_l, dim=-1)\n        \n        # Step 2: Landmark to landmark attention (for inverse approximation)\n        q_landmarks = torch.gather(q, 2, topk_idx_expanded)  # [B, H, m, d_h]\n        attn_l_to_l = torch.matmul(q_landmarks, k_landmarks.transpose(-2, -1)) * self.scale  # [B, H, m, m]\n        attn_l_to_l = F.softmax(attn_l_to_l, dim=-1)\n        \n        # Step 3: Approximate inverse using iterative method\n        # For numerical stability, use pseudo-inverse approximation\n        attn_l_to_l_inv = self._iterative_pinv(attn_l_to_l)  # [B, H, m, m]\n        \n        # Step 4: Landmark to Q attention (for reconstruction)\n        attn_l_to_q = torch.matmul(q_landmarks, k.transpose(-2, -1)) * self.scale  # [B, H, m, N]\n        attn_l_to_q = F.softmax(attn_l_to_q, dim=-1)\n        \n        # Nyström formula: Attn ≈ attn_q_to_l @ attn_l_to_l_inv @ attn_l_to_q\n        attn_approx = torch.matmul(attn_q_to_l, torch.matmul(attn_l_to_l_inv, attn_l_to_q))  # [B, H, N, N]\n        \n        # Normalize\n        attn_approx = attn_approx / (attn_approx.sum(dim=-1, keepdim=True) + 1e-8)\n        attn_approx = self.attn_dropout(attn_approx)\n        \n        # Apply attention to values\n        out = torch.matmul(attn_approx, v)  # [B, H, N, d_h]\n        out = out.transpose(1, 2).reshape(B, N, C)\n        out = self.proj_dropout(self.out_proj(out))\n        \n        if return_attn:\n            return out, attn_approx.detach()\n        return out, None\n    \n    def _iterative_pinv(self, A, num_iter=6):\n        \"\"\"\n        Iterative pseudo-inverse using Newton-Schulz iteration.\n        More stable than direct matrix inversion.\n        \"\"\"\n        # Initial approximation\n        A_t = A.transpose(-2, -1)\n        norm_A = torch.norm(A, dim=(-2, -1), keepdim=True)\n        A_normalized = A / (norm_A + 1e-8)\n        \n        # Newton-Schulz iteration: X_{k+1} = X_k @ (2I - A @ X_k)\n        I = torch.eye(A.size(-1), device=A.device).unsqueeze(0).unsqueeze(0)\n        X = A_t / (norm_A + 1e-8)\n        \n        for _ in range(num_iter):\n            X = X @ (2 * I - A_normalized @ X)\n        \n        return X / (norm_A + 1e-8)\n    \n    def predict_attention(self, x):\n        \"\"\"Predict attention map for distillation loss.\"\"\"\n        B, N, C = x.shape\n        m = min(self.num_landmarks, N)\n        \n        scores = self.landmark_scorer(x).squeeze(-1)\n        topk_vals, topk_idx = torch.topk(scores, m, dim=-1)\n        \n        q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        topk_idx_expanded = topk_idx.unsqueeze(1).unsqueeze(-1).expand(-1, self.num_heads, -1, self.head_dim)\n        k_landmarks = torch.gather(k, 2, topk_idx_expanded)\n        \n        attn_q_to_l = torch.matmul(q, k_landmarks.transpose(-2, -1)) * self.scale\n        attn_q_to_l = F.softmax(attn_q_to_l, dim=-1)\n        \n        q_landmarks = torch.gather(q, 2, topk_idx_expanded)\n        attn_l_to_l = torch.matmul(q_landmarks, k_landmarks.transpose(-2, -1)) * self.scale\n        attn_l_to_l = F.softmax(attn_l_to_l, dim=-1)\n        \n        attn_l_to_l_inv = self._iterative_pinv(attn_l_to_l)\n        \n        attn_l_to_q = torch.matmul(q_landmarks, k.transpose(-2, -1)) * self.scale\n        attn_l_to_q = F.softmax(attn_l_to_q, dim=-1)\n        \n        attn_approx = torch.matmul(attn_q_to_l, torch.matmul(attn_l_to_l_inv, attn_l_to_q))\n        attn_approx = attn_approx / (attn_approx.sum(dim=-1, keepdim=True) + 1e-8)\n        \n        return attn_approx\n    \n    def get_landmark_scores(self, x):\n        \"\"\"Get landmark scores for analysis.\"\"\"\n        return self.landmark_scorer(x).squeeze(-1)\n\n\nclass LNABlock(nn.Module):\n    \"\"\"Transformer block with Learned Nyström Attention.\"\"\"\n    \n    def __init__(self, dim: int, num_heads: int, num_landmarks: int, mlp_ratio: float, dropout: float = 0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = LearnedNystromAttention(dim, num_heads, num_landmarks, dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n    \n    def forward(self, x, return_attn=False):\n        attn_out, attn_map = self.attn(self.norm1(x), return_attn)\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x, attn_map\n\n\nclass LNAViT(nn.Module):\n    \"\"\"Model C: ViT with Learned Nyström Attention.\"\"\"\n    \n    def __init__(self, img_size: int, num_classes: int, cfg: Config, num_landmarks: int = None):\n        super().__init__()\n        \n        num_landmarks = num_landmarks or cfg.NUM_LANDMARKS\n        \n        self.patch_embed = PatchEmbedding(img_size, cfg.PATCH_SIZE, 3, cfg.DIM)\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.DIM))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, cfg.DIM))\n        self.pos_dropout = nn.Dropout(cfg.DROPOUT)\n        \n        self.blocks = nn.ModuleList([\n            LNABlock(cfg.DIM, cfg.HEADS, num_landmarks, cfg.MLP_RATIO, cfg.DROPOUT)\n            for _ in range(cfg.DEPTH)\n        ])\n        \n        self.norm = nn.LayerNorm(cfg.DIM)\n        self.head = nn.Linear(cfg.DIM, num_classes)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        self.apply(self._init_module)\n    \n    def _init_module(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n    \n    def get_embeddings(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        return self.pos_dropout(x + self.pos_embed)\n    \n    def get_all_attention_maps(self, x):\n        \"\"\"Get predicted attention maps for distillation.\"\"\"\n        x = self.get_embeddings(x)\n        attn_maps = []\n        for block in self.blocks:\n            attn = block.attn.predict_attention(block.norm1(x))\n            attn_maps.append(attn)\n            x, _ = block(x, return_attn=False)\n        return attn_maps\n    \n    def forward(self, x, return_attn=False):\n        x = self.get_embeddings(x)\n        \n        attn_maps = []\n        for block in self.blocks:\n            x, attn = block(x, return_attn)\n            if return_attn and attn is not None:\n                attn_maps.append(attn)\n        \n        x = self.norm(x)\n        logits = self.head(x[:, 0])\n        \n        if return_attn:\n            return logits, attn_maps\n        return logits, None\n    \n    def freeze_classifier(self):\n        self.head.requires_grad_(False)\n        self.norm.requires_grad_(False)\n    \n    def unfreeze_all(self):\n        for param in self.parameters():\n            param.requires_grad = True\n\n\n# ================================================================================\n# SECTION 7: TRAINING UTILITIES\n# ================================================================================\n\ndef get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step) / float(max(1, warmup_steps))\n        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\n@torch.no_grad()\ndef evaluate(model, loader, cfg):\n    model.eval()\n    correct = 0\n    total = 0\n    total_loss = 0\n    \n    for images, targets in loader:\n        images, targets = images.to(cfg.DEVICE), targets.to(cfg.DEVICE)\n        outputs, _ = model(images)\n        loss = F.cross_entropy(outputs, targets)\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total\n\n\ndef compute_attention_distillation_loss(student_attns, teacher_attns):\n    \"\"\"Compute MSE + KL loss between attention maps.\"\"\"\n    total_mse = 0\n    total_kl = 0\n    \n    for s_attn, t_attn in zip(student_attns, teacher_attns):\n        mse = F.mse_loss(s_attn, t_attn)\n        total_mse += mse\n        \n        kl = F.kl_div(\n            torch.log(s_attn + 1e-8),\n            t_attn,\n            reduction='batchmean'\n        )\n        total_kl += kl\n    \n    num_layers = len(student_attns)\n    return total_mse / num_layers, total_kl / num_layers\n\n\n# ================================================================================\n# SECTION 8: PHASE 1 - TRAIN TEACHER\n# ================================================================================\n\ndef train_teacher(teacher, train_loader, test_loader, cfg):\n    subheader(\"Phase 1: Training Teacher (Standard ViT)\")\n    \n    teacher = teacher.to(cfg.DEVICE)\n    optimizer = torch.optim.AdamW(teacher.parameters(), lr=cfg.LR, weight_decay=cfg.WD)\n    \n    total_steps = cfg.EPOCHS_TEACHER * len(train_loader)\n    warmup_steps = cfg.WARMUP_EPOCHS * len(train_loader)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    scaler = GradScaler()\n    \n    best_acc = 0\n    \n    for epoch in range(cfg.EPOCHS_TEACHER):\n        teacher.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, targets in train_loader:\n            images, targets = images.to(cfg.DEVICE), targets.to(cfg.DEVICE)\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                outputs, _ = teacher(images)\n                loss = F.cross_entropy(outputs, targets)\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(teacher.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        test_loss, test_acc = evaluate(teacher, test_loader, cfg)\n        best_acc = max(best_acc, test_acc)\n        \n        print(f\"    Epoch {epoch+1:2d}/{cfg.EPOCHS_TEACHER} | \"\n              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n              f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n              f\"Best: {best_acc:.2f}%\")\n    \n    print(f\"\\n  Teacher Training Complete. Best Accuracy: {best_acc:.2f}%\")\n    return best_acc\n\n\n# ================================================================================\n# SECTION 9: PHASE 2 - DISTILLATION\n# ================================================================================\n\ndef train_distillation(student, teacher, train_loader, test_loader, cfg, student_name=\"Student\"):\n    subheader(f\"Phase 2: Distillation ({student_name})\")\n    \n    student = student.to(cfg.DEVICE)\n    teacher = teacher.to(cfg.DEVICE)\n    \n    teacher.eval()\n    for param in teacher.parameters():\n        param.requires_grad = False\n    \n    student.freeze_classifier()\n    \n    trainable_params = [p for p in student.parameters() if p.requires_grad]\n    print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n    \n    optimizer = torch.optim.AdamW(trainable_params, lr=cfg.LR_DISTILL, weight_decay=cfg.WD)\n    \n    total_steps = cfg.EPOCHS_DISTILL * len(train_loader)\n    warmup_steps = 2 * len(train_loader)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    \n    best_corr = 0\n    \n    for epoch in range(cfg.EPOCHS_DISTILL):\n        student.train()\n        epoch_mse = 0\n        epoch_kl = 0\n        num_batches = 0\n        \n        for images, _ in train_loader:\n            images = images.to(cfg.DEVICE)\n            \n            with torch.no_grad():\n                _, teacher_attns = teacher(images, return_attn=True)\n            \n            student_attns = student.get_all_attention_maps(images)\n            \n            mse_loss, kl_loss = compute_attention_distillation_loss(student_attns, teacher_attns)\n            loss = mse_loss + 0.1 * kl_loss\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            epoch_mse += mse_loss.item()\n            epoch_kl += kl_loss.item()\n            num_batches += 1\n        \n        avg_mse = epoch_mse / num_batches\n        avg_kl = epoch_kl / num_batches\n        \n        # Evaluate correlation\n        student.eval()\n        correlations = []\n        \n        with torch.no_grad():\n            for batch_idx, (images, _) in enumerate(test_loader):\n                if batch_idx >= 5:\n                    break\n                    \n                images = images.to(cfg.DEVICE)\n                _, teacher_attns = teacher(images, return_attn=True)\n                _, student_attns = student(images, return_attn=True)\n                \n                t_attn = torch.stack(teacher_attns).mean(0)\n                s_attn = torch.stack(student_attns).mean(0)\n                \n                t_flat = t_attn.view(-1).cpu().numpy()\n                s_flat = s_attn.view(-1).cpu().numpy()\n                corr, _ = pearsonr(t_flat, s_flat)\n                if not np.isnan(corr):\n                    correlations.append(corr)\n        \n        avg_corr = np.mean(correlations) if correlations else 0\n        best_corr = max(best_corr, avg_corr)\n        \n        print(f\"    Epoch {epoch+1:2d}/{cfg.EPOCHS_DISTILL} | \"\n              f\"MSE: {avg_mse:.6f} | KL: {avg_kl:.4f} | \"\n              f\"Corr: {avg_corr:.4f} | Best: {best_corr:.4f}\")\n    \n    student.unfreeze_all()\n    \n    print(f\"\\n  Distillation Complete. Best Correlation: {best_corr:.4f}\")\n    return best_corr\n\n\n# ================================================================================\n# SECTION 10: PHASE 3 - TRAIN STUDENT CLASSIFICATION\n# ================================================================================\n\ndef train_student_classification(student, teacher, train_loader, test_loader, cfg, \n                                  student_name=\"Student\", use_distill_loss=True):\n    subheader(f\"Phase 3: Classification Training ({student_name})\")\n    \n    student = student.to(cfg.DEVICE)\n    teacher = teacher.to(cfg.DEVICE)\n    teacher.eval()\n    \n    for param in teacher.parameters():\n        param.requires_grad = False\n    \n    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.LR, weight_decay=cfg.WD)\n    \n    total_steps = cfg.EPOCHS_STUDENT * len(train_loader)\n    warmup_steps = cfg.WARMUP_EPOCHS * len(train_loader)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    scaler = GradScaler()\n    \n    best_acc = 0\n    \n    for epoch in range(cfg.EPOCHS_STUDENT):\n        student.train()\n        total_loss = 0\n        total_task_loss = 0\n        total_distill_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, targets in train_loader:\n            images, targets = images.to(cfg.DEVICE), targets.to(cfg.DEVICE)\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                outputs, student_attns = student(images, return_attn=True)\n                task_loss = F.cross_entropy(outputs, targets)\n                \n                if use_distill_loss and epoch < cfg.EPOCHS_STUDENT // 2:\n                    with torch.no_grad():\n                        _, teacher_attns = teacher(images, return_attn=True)\n                    \n                    mse_loss, kl_loss = compute_attention_distillation_loss(\n                        student_attns, teacher_attns\n                    )\n                    distill_loss = mse_loss + 0.1 * kl_loss\n                    \n                    distill_weight = cfg.DISTILL_LAMBDA * (1 - epoch / (cfg.EPOCHS_STUDENT // 2))\n                    loss = task_loss + distill_weight * distill_loss\n                    total_distill_loss += distill_loss.item()\n                else:\n                    loss = task_loss\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            \n            total_loss += loss.item()\n            total_task_loss += task_loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        test_loss, test_acc = evaluate(student, test_loader, cfg)\n        best_acc = max(best_acc, test_acc)\n        \n        print(f\"    Epoch {epoch+1:2d}/{cfg.EPOCHS_STUDENT} | \"\n              f\"Loss: {train_loss:.4f} | \"\n              f\"Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | Best: {best_acc:.2f}%\")\n    \n    print(f\"\\n  {student_name} Training Complete. Best Accuracy: {best_acc:.2f}%\")\n    return best_acc\n\n\n# ================================================================================\n# SECTION 11: ATTENTION FIDELITY METRICS\n# ================================================================================\n\n@torch.no_grad()\ndef compute_attention_fidelity(teacher, student, loader, cfg, num_batches=10):\n    teacher.eval()\n    student.eval()\n    \n    correlations = []\n    topk_overlaps = []\n    mse_values = []\n    kl_values = []\n    \n    for batch_idx, (images, _) in enumerate(loader):\n        if batch_idx >= num_batches:\n            break\n            \n        images = images.to(cfg.DEVICE)\n        \n        _, teacher_attns = teacher(images, return_attn=True)\n        _, student_attns = student(images, return_attn=True)\n        \n        teacher_attn = torch.stack(teacher_attns).mean(0)\n        student_attn = torch.stack(student_attns).mean(0)\n        \n        # Pearson Correlation\n        t_flat = teacher_attn.view(-1).cpu().numpy()\n        s_flat = student_attn.view(-1).cpu().numpy()\n        corr, _ = pearsonr(t_flat, s_flat)\n        if not np.isnan(corr):\n            correlations.append(corr)\n        \n        # Top-K Overlap\n        k = 5\n        B, H, N, _ = teacher_attn.shape\n        overlap_sum = 0\n        count = 0\n        for b in range(min(4, B)):\n            for h in range(H):\n                for i in range(min(16, N)):\n                    t_topk = set(torch.topk(teacher_attn[b, h, i], k).indices.tolist())\n                    s_topk = set(torch.topk(student_attn[b, h, i], k).indices.tolist())\n                    overlap_sum += len(t_topk & s_topk) / k\n                    count += 1\n        topk_overlaps.append(overlap_sum / count if count > 0 else 0)\n        \n        # MSE\n        mse = F.mse_loss(student_attn, teacher_attn).item()\n        mse_values.append(mse)\n        \n        # KL Divergence\n        kl = F.kl_div(\n            torch.log(student_attn + 1e-8), \n            teacher_attn, \n            reduction='batchmean'\n        ).item()\n        kl_values.append(kl)\n    \n    return {\n        'correlation': np.mean(correlations) if correlations else 0,\n        'correlation_std': np.std(correlations) if len(correlations) > 1 else 0,\n        'topk_overlap': np.mean(topk_overlaps),\n        'topk_overlap_std': np.std(topk_overlaps) if len(topk_overlaps) > 1 else 0,\n        'mse': np.mean(mse_values),\n        'mse_std': np.std(mse_values) if len(mse_values) > 1 else 0,\n        'kl_divergence': np.mean(kl_values),\n        'kl_divergence_std': np.std(kl_values) if len(kl_values) > 1 else 0\n    }\n\n\n@torch.no_grad()\ndef save_attention_maps(models_dict, loader, cfg, save_path=\"attention_maps\"):\n    os.makedirs(save_path, exist_ok=True)\n    \n    for model in models_dict.values():\n        model.eval()\n    \n    images, labels = next(iter(loader))\n    images = images[:4].to(cfg.DEVICE)\n    \n    for name, model in models_dict.items():\n        _, attns = model(images, return_attn=True)\n        \n        for layer_idx, attn in enumerate(attns):\n            np.save(os.path.join(save_path, f\"{name}_layer{layer_idx}.npy\"), attn.cpu().numpy())\n        \n        avg_attn = torch.stack(attns).mean(0).cpu().numpy()\n        np.save(os.path.join(save_path, f\"{name}_attention_avg.npy\"), avg_attn)\n    \n    np.save(os.path.join(save_path, \"images.npy\"), images.cpu().numpy())\n    np.save(os.path.join(save_path, \"labels.npy\"), labels[:4].numpy())\n    \n    print(f\"    Attention maps saved to {save_path}/\")\n\n\n# ================================================================================\n# SECTION 12: THEORETICAL COMPLEXITY ANALYSIS\n# ================================================================================\n\ndef compute_theoretical_complexity(cfg: Config, num_tokens: int):\n    n = num_tokens\n    d = cfg.DIM\n    H = cfg.HEADS\n    L = cfg.DEPTH\n    head_dim = d // H\n    \n    # MSKA parameters\n    ranks = cfg.MSKA_RANKS\n    r_max = max(ranks)\n    num_scales = len(ranks)\n    \n    # LNA parameters\n    m = cfg.NUM_LANDMARKS\n    \n    # Standard Attention\n    std_qkv = 3 * n * d * d\n    std_attn_matrix = n * n * d\n    std_attn_v = n * n * d\n    std_out = n * d * d\n    std_mlp = 2 * n * d * int(d * cfg.MLP_RATIO)\n    std_total = std_qkv + std_attn_matrix + std_attn_v + std_out + std_mlp\n    \n    # MSKA (Multi-Scale Kernel Attention)\n    mska_phi_psi = sum(2 * n * d * (r * H) for r in ranks)  # All scales\n    mska_attns = sum(n * n * H for _ in ranks)  # Attention at each scale\n    mska_scale_select = n * d + d * num_scales  # Scale selector\n    mska_v = n * d * d\n    mska_out = n * d * d\n    mska_mlp = 2 * n * d * int(d * cfg.MLP_RATIO)\n    mska_total = mska_phi_psi + mska_attns + mska_scale_select + mska_v + mska_out + mska_mlp\n    \n    # LNA (Learned Nyström Attention)\n    lna_qkv = 3 * n * d * d\n    lna_landmark_score = n * d + d * 1\n    lna_attn_q_to_l = n * m * d  # Q to landmarks\n    lna_attn_l_to_l = m * m * d  # Landmarks to landmarks\n    lna_attn_l_to_q = m * n * d  # Landmarks to all\n    lna_pinv = m * m * m * 6  # Iterative inverse (6 iterations)\n    lna_final = n * n * H  # Final attention (reconstructed)\n    lna_out = n * d * d\n    lna_mlp = 2 * n * d * int(d * cfg.MLP_RATIO)\n    lna_total = lna_qkv + lna_landmark_score + lna_attn_q_to_l + lna_attn_l_to_l + lna_attn_l_to_q + lna_pinv + lna_final + lna_out + lna_mlp\n    \n    return {\n        'standard': {\n            'per_layer': std_total,\n            'total': std_total * L,\n            'complexity': f\"O(n²d) = O({n}² × {d}) = O({n*n*d})\"\n        },\n        'mska': {\n            'per_layer': mska_total,\n            'total': mska_total * L,\n            'complexity': f\"O(n² × S) where S={num_scales} scales, r_max={r_max}\",\n            'reduction_vs_std': (1 - mska_total / std_total) * 100,\n            'speedup_vs_std': std_total / mska_total\n        },\n        'lna': {\n            'per_layer': lna_total,\n            'total': lna_total * L,\n            'complexity': f\"O(n × m²) = O({n} × {m}²) = O({n*m*m})\",\n            'reduction_vs_std': (1 - lna_total / std_total) * 100,\n            'speedup_vs_std': std_total / lna_total\n        },\n        'num_tokens': n\n    }\n\n\n# ================================================================================\n# SECTION 13: STATISTICAL TESTS\n# ================================================================================\n\ndef compute_statistics(accs_list: List[List[float]], names: List[str]):\n    \"\"\"Compute statistics for multiple models.\"\"\"\n    \n    results = {}\n    \n    for accs, name in zip(accs_list, names):\n        mean = np.mean(accs)\n        std = np.std(accs, ddof=1) if len(accs) > 1 else 0\n        stderr = sem(accs) if len(accs) > 1 else 0\n        \n        t_critical = 4.303 if len(accs) == 3 else 2.776\n        ci_95 = (mean - t_critical * stderr, mean + t_critical * stderr)\n        \n        results[name] = {\n            'mean': mean,\n            'std': std,\n            'sem': stderr,\n            'ci_95': ci_95,\n            'all_seeds': accs\n        }\n    \n    # Pairwise t-tests\n    pairwise_tests = {}\n    for i in range(len(accs_list)):\n        for j in range(i + 1, len(accs_list)):\n            if len(accs_list[i]) > 1 and len(accs_list[j]) > 1:\n                t_stat, p_value = ttest_rel(accs_list[i], accs_list[j])\n                \n                pooled_std = np.sqrt((results[names[i]]['std']**2 + results[names[j]]['std']**2) / 2)\n                cohens_d = (results[names[i]]['mean'] - results[names[j]]['mean']) / pooled_std if pooled_std > 0 else 0\n                \n                pairwise_tests[f\"{names[i]}_vs_{names[j]}\"] = {\n                    't_statistic': t_stat,\n                    'p_value': p_value,\n                    'significant_005': p_value < 0.05,\n                    'cohens_d': cohens_d\n                }\n    \n    return results, pairwise_tests\n\n\n# ================================================================================\n# SECTION 14: SCALE WEIGHT ANALYSIS (MSKA-specific)\n# ================================================================================\n\n@torch.no_grad()\ndef analyze_scale_weights(model, loader, cfg, num_batches=10):\n    \"\"\"Analyze which scales MSKA uses for different inputs.\"\"\"\n    model.eval()\n    \n    all_weights = []\n    \n    for batch_idx, (images, _) in enumerate(loader):\n        if batch_idx >= num_batches:\n            break\n            \n        images = images.to(cfg.DEVICE)\n        layer_weights = model.get_scale_weights(images)\n        \n        # Average across layers\n        avg_weights = torch.stack(layer_weights).mean(0)  # [B, num_scales]\n        all_weights.append(avg_weights.cpu())\n    \n    all_weights = torch.cat(all_weights, dim=0)  # [total_samples, num_scales]\n    \n    return {\n        'mean_weights': all_weights.mean(0).numpy(),\n        'std_weights': all_weights.std(0).numpy(),\n        'scale_ranks': cfg.MSKA_RANKS\n    }\n\n\n# ================================================================================\n# SECTION 15: MAIN EXPERIMENT RUNNER\n# ================================================================================\n\ndef run_full_experiment(dataset_name: str, get_data_fn, cfg: Config):\n    header(f\"EXPERIMENT: {dataset_name.upper()}\")\n    \n    print(f\"\\n  Loading {dataset_name} ({cfg.DATA_FRACTION*100:.0f}% of data)...\")\n    train_loader, test_loader, num_classes, img_size = get_data_fn(cfg)\n    print(f\"  Classes: {num_classes}, Image size: {img_size}x{img_size}\")\n    print(f\"  Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n    \n    num_patches = (img_size // cfg.PATCH_SIZE) ** 2\n    num_tokens = num_patches + 1\n    print(f\"  Number of tokens: {num_tokens}\")\n    \n    # Results storage\n    teacher_accs = []\n    mska_accs = []\n    lna_accs = []\n    mska_distill_corrs = []\n    lna_distill_corrs = []\n    mska_fidelities = []\n    lna_fidelities = []\n    \n    for seed_idx, seed in enumerate(cfg.SEEDS):\n        subheader(f\"Seed {seed_idx+1}/{len(cfg.SEEDS)}: {seed}\")\n        set_seed(seed)\n        \n        # Create models\n        teacher = StandardViT(img_size, num_classes, cfg).to(cfg.DEVICE)\n        student_mska = MSKAViT(img_size, num_classes, cfg).to(cfg.DEVICE)\n        student_lna = LNAViT(img_size, num_classes, cfg).to(cfg.DEVICE)\n        \n        print(f\"\\n  Model Parameters:\")\n        print(f\"    Teacher (Standard):    {fmt_params(count_params(teacher))}\")\n        print(f\"    MSKA Student:          {fmt_params(count_params(student_mska))}\")\n        print(f\"    LNA Student:           {fmt_params(count_params(student_lna))}\")\n        \n        # Phase 1: Train Teacher\n        teacher_acc = train_teacher(teacher, train_loader, test_loader, cfg)\n        teacher_accs.append(teacher_acc)\n        \n        # Phase 2a: Distill to MSKA\n        mska_corr = train_distillation(student_mska, teacher, train_loader, test_loader, cfg, \"MSKA\")\n        mska_distill_corrs.append(mska_corr)\n        \n        # Phase 2b: Distill to LNA\n        lna_corr = train_distillation(student_lna, teacher, train_loader, test_loader, cfg, \"LNA\")\n        lna_distill_corrs.append(lna_corr)\n        \n        # Phase 3a: Train MSKA for classification\n        mska_acc = train_student_classification(student_mska, teacher, train_loader, test_loader, cfg, \"MSKA\", use_distill_loss=True)\n        mska_accs.append(mska_acc)\n        \n        # Phase 3b: Train LNA for classification\n        lna_acc = train_student_classification(student_lna, teacher, train_loader, test_loader, cfg, \"LNA\", use_distill_loss=True)\n        lna_accs.append(lna_acc)\n        \n        # Compute fidelity\n        print(f\"\\n  Computing Attention Fidelity...\")\n        mska_fid = compute_attention_fidelity(teacher, student_mska, test_loader, cfg)\n        lna_fid = compute_attention_fidelity(teacher, student_lna, test_loader, cfg)\n        mska_fidelities.append(mska_fid)\n        lna_fidelities.append(lna_fid)\n        \n        print(f\"\\n  MSKA Fidelity: Corr={mska_fid['correlation']:.4f}, TopK={mska_fid['topk_overlap']:.4f}\")\n        print(f\"  LNA Fidelity:  Corr={lna_fid['correlation']:.4f}, TopK={lna_fid['topk_overlap']:.4f}\")\n        \n        # Analyze MSKA scale weights (first seed only)\n        if seed_idx == 0:\n            print(f\"\\n  Analyzing MSKA Scale Weights...\")\n            scale_analysis = analyze_scale_weights(student_mska, test_loader, cfg)\n            print(f\"    Scale Ranks: {scale_analysis['scale_ranks']}\")\n            print(f\"    Mean Weights: {scale_analysis['mean_weights']}\")\n            print(f\"    Std Weights:  {scale_analysis['std_weights']}\")\n            \n            # Save attention maps\n            save_path = f\"attention_maps_{dataset_name.lower().replace('-', '_')}\"\n            save_attention_maps(\n                {'teacher': teacher, 'mska': student_mska, 'lna': student_lna},\n                test_loader, cfg, save_path\n            )\n    \n    # Statistical Analysis\n    subheader(\"Statistical Analysis\")\n    \n    stats, pairwise = compute_statistics(\n        [teacher_accs, mska_accs, lna_accs],\n        ['Teacher', 'MSKA', 'LNA']\n    )\n    \n    print(f\"\\n  Model Accuracies:\")\n    for name in ['Teacher', 'MSKA', 'LNA']:\n        s = stats[name]\n        print(f\"    {name:10s}: {s['mean']:.2f}% ± {s['std']:.2f}% | CI: ({s['ci_95'][0]:.2f}%, {s['ci_95'][1]:.2f}%)\")\n    \n    print(f\"\\n  Pairwise Comparisons:\")\n    for pair, test in pairwise.items():\n        print(f\"    {pair:20s}: p={test['p_value']:.6f}, d={test['cohens_d']:.4f}, sig={test['significant_005']}\")\n    \n    # Attention Fidelity Summary\n    subheader(\"Attention Fidelity Summary\")\n    \n    mska_fid_avg = {\n        'correlation': np.mean([f['correlation'] for f in mska_fidelities]),\n        'topk_overlap': np.mean([f['topk_overlap'] for f in mska_fidelities]),\n        'mse': np.mean([f['mse'] for f in mska_fidelities]),\n        'kl_divergence': np.mean([f['kl_divergence'] for f in mska_fidelities])\n    }\n    \n    lna_fid_avg = {\n        'correlation': np.mean([f['correlation'] for f in lna_fidelities]),\n        'topk_overlap': np.mean([f['topk_overlap'] for f in lna_fidelities]),\n        'mse': np.mean([f['mse'] for f in lna_fidelities]),\n        'kl_divergence': np.mean([f['kl_divergence'] for f in lna_fidelities])\n    }\n    \n    print(f\"  MSKA: Corr={mska_fid_avg['correlation']:.4f}, TopK={mska_fid_avg['topk_overlap']:.4f}, MSE={mska_fid_avg['mse']:.6f}\")\n    print(f\"  LNA:  Corr={lna_fid_avg['correlation']:.4f}, TopK={lna_fid_avg['topk_overlap']:.4f}, MSE={lna_fid_avg['mse']:.6f}\")\n    \n    # Complexity Analysis\n    subheader(\"Theoretical Complexity\")\n    \n    complexity = compute_theoretical_complexity(cfg, num_tokens)\n    \n    print(f\"\\n  Standard Attention:\")\n    print(f\"    Complexity: {complexity['standard']['complexity']}\")\n    print(f\"    Total FLOPs: {complexity['standard']['total']:,}\")\n    \n    print(f\"\\n  MSKA (Multi-Scale Kernel):\")\n    print(f\"    Complexity: {complexity['mska']['complexity']}\")\n    print(f\"    Total FLOPs: {complexity['mska']['total']:,}\")\n    print(f\"    Reduction: {complexity['mska']['reduction_vs_std']:.1f}%\")\n    print(f\"    Speedup: {complexity['mska']['speedup_vs_std']:.2f}x\")\n    \n    print(f\"\\n  LNA (Learned Nyström):\")\n    print(f\"    Complexity: {complexity['lna']['complexity']}\")\n    print(f\"    Total FLOPs: {complexity['lna']['total']:,}\")\n    print(f\"    Reduction: {complexity['lna']['reduction_vs_std']:.1f}%\")\n    print(f\"    Speedup: {complexity['lna']['speedup_vs_std']:.2f}x\")\n    \n    return {\n        'dataset': dataset_name,\n        'teacher_accs': teacher_accs,\n        'mska_accs': mska_accs,\n        'lna_accs': lna_accs,\n        'statistics': stats,\n        'pairwise_tests': pairwise,\n        'mska_fidelity': mska_fid_avg,\n        'lna_fidelity': lna_fid_avg,\n        'complexity': complexity\n    }\n\n\n# ================================================================================\n# SECTION 16: ABLATION STUDIES\n# ================================================================================\n\ndef run_ablation_study(train_loader, test_loader, num_classes, img_size, cfg):\n    header(\"ABLATION STUDIES\")\n    \n    results = {'mska_scales': {}, 'lna_landmarks': {}}\n    \n    # MSKA scales ablation\n    subheader(\"Ablation: MSKA Scale Configurations\")\n    \n    for scales in cfg.ABLATION_MSKA_SCALES:\n        set_seed(cfg.SEEDS[0])\n        scales_str = str(scales)\n        print(f\"\\n  Testing scales = {scales}\")\n        \n        teacher = StandardViT(img_size, num_classes, cfg).to(cfg.DEVICE)\n        student = MSKAViT(img_size, num_classes, cfg, ranks=scales).to(cfg.DEVICE)\n        \n        teacher_acc = train_teacher(teacher, train_loader, test_loader, cfg)\n        train_distillation(student, teacher, train_loader, test_loader, cfg, f\"MSKA-{scales}\")\n        student_acc = train_student_classification(student, teacher, train_loader, test_loader, cfg, f\"MSKA-{scales}\", False)\n        \n        fid = compute_attention_fidelity(teacher, student, test_loader, cfg, num_batches=5)\n        \n        results['mska_scales'][scales_str] = {\n            'accuracy': student_acc,\n            'correlation': fid['correlation'],\n            'gap': teacher_acc - student_acc\n        }\n        \n        print(f\"  Scales {scales}: Acc={student_acc:.2f}%, Corr={fid['correlation']:.4f}\")\n    \n    # LNA landmarks ablation\n    subheader(\"Ablation: LNA Number of Landmarks\")\n    \n    for m in cfg.ABLATION_NUM_LANDMARKS:\n        set_seed(cfg.SEEDS[0])\n        print(f\"\\n  Testing landmarks = {m}\")\n        \n        teacher = StandardViT(img_size, num_classes, cfg).to(cfg.DEVICE)\n        student = LNAViT(img_size, num_classes, cfg, num_landmarks=m).to(cfg.DEVICE)\n        \n        teacher_acc = train_teacher(teacher, train_loader, test_loader, cfg)\n        train_distillation(student, teacher, train_loader, test_loader, cfg, f\"LNA-m{m}\")\n        student_acc = train_student_classification(student, teacher, train_loader, test_loader, cfg, f\"LNA-m{m}\", False)\n        \n        fid = compute_attention_fidelity(teacher, student, test_loader, cfg, num_batches=5)\n        \n        results['lna_landmarks'][m] = {\n            'accuracy': student_acc,\n            'correlation': fid['correlation'],\n            'gap': teacher_acc - student_acc\n        }\n        \n        print(f\"  Landmarks {m}: Acc={student_acc:.2f}%, Corr={fid['correlation']:.4f}\")\n    \n    return results\n\n\n# ================================================================================\n# SECTION 17: MAIN ENTRY POINT\n# ================================================================================\n\ndef main():\n    print(\"\\n\" + \"=\" * 88)\n    print(\"LEARNED ATTENTION DISTILLATION: TWO NOVEL APPROACHES\".center(88))\n    print(\"=\" * 88)\n    \n    print(\"\"\"\n    ╔══════════════════════════════════════════════════════════════════════════════╗\n    ║  MODELS COMPARED                                                             ║\n    ╠══════════════════════════════════════════════════════════════════════════════╣\n    ║  Teacher:  Standard ViT (O(n²) attention)                                    ║\n    ║                                                                              ║\n    ║  Student 1: Multi-Scale Kernel Attention (MSKA)                              ║\n    ║             Novel: Multiple parallel kernels at ranks [16, 64, 128]          ║\n    ║             Novel: Dynamic scale mixing learned via distillation             ║\n    ║             Complexity: O(n × r_max²) with learned scale selection           ║\n    ║                                                                              ║\n    ║  Student 2: Learned Nyström Attention (LNA)                                  ║\n    ║             Novel: Landmarks are LEARNED (not random like original Nyström)  ║\n    ║             Novel: Landmark positions optimized via attention distillation   ║\n    ║             Complexity: O(n × m²) where m = num_landmarks << n               ║\n    ╚══════════════════════════════════════════════════════════════════════════════╝\n    \"\"\")\n    \n    cfg = Config()\n    \n    print(f\"  Device: {cfg.DEVICE}\")\n    print(f\"  Seeds: {cfg.SEEDS}\")\n    print(f\"  Data Fraction: {cfg.DATA_FRACTION*100:.0f}%\")\n    print(f\"  MSKA Ranks: {cfg.MSKA_RANKS}\")\n    print(f\"  LNA Landmarks: {cfg.NUM_LANDMARKS}\")\n    \n    all_results = {}\n    \n    # CIFAR-10\n    cifar10_results = run_full_experiment(\"CIFAR-10\", get_cifar10, cfg)\n    all_results['cifar10'] = cifar10_results\n    \n    # CIFAR-100\n    cifar100_results = run_full_experiment(\"CIFAR-100\", get_cifar100, cfg)\n    all_results['cifar100'] = cifar100_results\n    \n    # Ablation\n    train_loader, test_loader, num_classes, img_size = get_cifar10(cfg)\n    ablation_results = run_ablation_study(train_loader, test_loader, num_classes, img_size, cfg)\n    all_results['ablation'] = ablation_results\n    \n    # FINAL SUMMARY\n    header(\"FINAL SUMMARY\")\n    \n    print(\"\\n\" + \"=\" * 88)\n    print(\"CLASSIFICATION ACCURACY COMPARISON\".center(88))\n    print(\"=\" * 88)\n    \n    for dataset in ['cifar10', 'cifar100']:\n        result = all_results[dataset]\n        stats = result['statistics']\n        \n        print(f\"\\n  {result['dataset']}:\")\n        print(f\"    Teacher:    {stats['Teacher']['mean']:.2f}% ± {stats['Teacher']['std']:.2f}%\")\n        print(f\"    MSKA:       {stats['MSKA']['mean']:.2f}% ± {stats['MSKA']['std']:.2f}%  (gap: {stats['Teacher']['mean'] - stats['MSKA']['mean']:.2f}%)\")\n        print(f\"    LNA:        {stats['LNA']['mean']:.2f}% ± {stats['LNA']['std']:.2f}%  (gap: {stats['Teacher']['mean'] - stats['LNA']['mean']:.2f}%)\")\n    \n    print(\"\\n\" + \"=\" * 88)\n    print(\"ATTENTION FIDELITY COMPARISON\".center(88))\n    print(\"=\" * 88)\n    \n    for dataset in ['cifar10', 'cifar100']:\n        result = all_results[dataset]\n        print(f\"\\n  {result['dataset']}:\")\n        print(f\"    MSKA: Corr={result['mska_fidelity']['correlation']:.4f}, TopK={result['mska_fidelity']['topk_overlap']:.4f}\")\n        print(f\"    LNA:  Corr={result['lna_fidelity']['correlation']:.4f}, TopK={result['lna_fidelity']['topk_overlap']:.4f}\")\n    \n    print(\"\\n\" + \"=\" * 88)\n    print(\"THEORETICAL COMPLEXITY\".center(88))\n    print(\"=\" * 88)\n    \n    for dataset in ['cifar10', 'cifar100']:\n        result = all_results[dataset]\n        comp = result['complexity']\n        print(f\"\\n  {result['dataset']}:\")\n        print(f\"    Standard: {comp['standard']['complexity']}\")\n        print(f\"    MSKA:     {comp['mska']['complexity']} (Reduction: {comp['mska']['reduction_vs_std']:.1f}%)\")\n        print(f\"    LNA:      {comp['lna']['complexity']} (Reduction: {comp['lna']['reduction_vs_std']:.1f}%)\")\n    \n    print(\"\\n\" + \"=\" * 88)\n    print(\"ABLATION STUDY RESULTS\".center(88))\n    print(\"=\" * 88)\n    \n    print(\"\\n  MSKA Scale Configurations:\")\n    for scales, res in all_results['ablation']['mska_scales'].items():\n        print(f\"    {scales}: Acc={res['accuracy']:.2f}%, Corr={res['correlation']:.4f}\")\n    \n    print(\"\\n  LNA Number of Landmarks:\")\n    for m, res in all_results['ablation']['lna_landmarks'].items():\n        print(f\"    m={m:3d}: Acc={res['accuracy']:.2f}%, Corr={res['correlation']:.4f}\")\n    \n    print(\"\\n\" + \"=\" * 88)\n    print(\"NOVEL CONTRIBUTIONS VALIDATED\".center(88))\n    print(\"=\" * 88)\n    \n    print(\"\"\"\n  ┌────────────────────────────────────────────────────────────────────────────────┐\n  │  1. MULTI-SCALE KERNEL ATTENTION (MSKA)                                        │\n  │     ✓ Multiple parallel kernels at different ranks [16, 64, 128]              │\n  │     ✓ Dynamic scale mixing: input-dependent blending of scales                │\n  │     ✓ Learns WHICH scale matters per input via distillation                   │\n  │     ✓ Unlike Performer (single fixed rank), we use adaptive multi-scale       │\n  │                                                                                │\n  │  2. LEARNED NYSTRÖM ATTENTION (LNA)                                            │\n  │     ✓ Landmarks are LEARNED parameters (not random sampling)                   │\n  │     ✓ Landmark positions optimized to match teacher attention patterns        │\n  │     ✓ Achieves O(n × m²) complexity where m << n                               │\n  │     ✓ Unlike original Nyström, our landmarks become \"attention hubs\"           │\n  │                                                                                │\n  │  3. BOTH STUDENTS                                                              │\n  │     ✓ Trained via proper 3-phase distillation pipeline                        │\n  │     ✓ Statistically validated with multiple seeds                              │\n  │     ✓ Comprehensive ablation studies                                           │\n  │     ✓ Attention maps saved for visualization                                   │\n  └────────────────────────────────────────────────────────────────────────────────┘\n    \"\"\")\n    \n    header(\"EXPERIMENT COMPLETE\")\n    \n    return all_results\n\n\nif __name__ == \"__main__\":\n    results = main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-20T08:47:02.701183Z","iopub.execute_input":"2026-02-20T08:47:02.701482Z","iopub.status.idle":"2026-02-20T11:32:07.917338Z","shell.execute_reply.started":"2026-02-20T08:47:02.701456Z","shell.execute_reply":"2026-02-20T11:32:07.916167Z"}},"outputs":[{"name":"stdout","text":"\n========================================================================================\n                  LEARNED ATTENTION DISTILLATION: TWO NOVEL APPROACHES                  \n========================================================================================\n\n    ╔══════════════════════════════════════════════════════════════════════════════╗\n    ║  MODELS COMPARED                                                             ║\n    ╠══════════════════════════════════════════════════════════════════════════════╣\n    ║  Teacher:  Standard ViT (O(n²) attention)                                    ║\n    ║                                                                              ║\n    ║  Student 1: Multi-Scale Kernel Attention (MSKA)                              ║\n    ║             Novel: Multiple parallel kernels at ranks [16, 64, 128]          ║\n    ║             Novel: Dynamic scale mixing learned via distillation             ║\n    ║             Complexity: O(n × r_max²) with learned scale selection           ║\n    ║                                                                              ║\n    ║  Student 2: Learned Nyström Attention (LNA)                                  ║\n    ║             Novel: Landmarks are LEARNED (not random like original Nyström)  ║\n    ║             Novel: Landmark positions optimized via attention distillation   ║\n    ║             Complexity: O(n × m²) where m = num_landmarks << n               ║\n    ╚══════════════════════════════════════════════════════════════════════════════╝\n    \n  Device: cuda\n  Seeds: [42, 123, 456]\n  Data Fraction: 50%\n  MSKA Ranks: [16, 64, 128]\n  LNA Landmarks: 16\n\n========================================================================================\n                                  EXPERIMENT: CIFAR-10                                  \n========================================================================================\n\n  Loading CIFAR-10 (50% of data)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:06<00:00, 27.8MB/s] \n","output_type":"stream"},{"name":"stdout","text":"  Classes: 10, Image size: 32x32\n  Train batches: 97, Test batches: 20\n  Number of tokens: 65\n\n----------------------------------------------------------------------------------------\n  Seed 1/3: 42\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.81M\n    MSKA Student:          4.36M\n    LNA Student:           1.88M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1700 | Train Acc: 18.69% | Test Loss: 1.9138 | Test Acc: 30.88% | Best: 30.88%\n    Epoch  2/10 | Train Loss: 2.0122 | Train Acc: 24.80% | Test Loss: 1.8203 | Test Acc: 32.02% | Best: 32.02%\n    Epoch  3/10 | Train Loss: 1.8598 | Train Acc: 31.47% | Test Loss: 1.6321 | Test Acc: 40.60% | Best: 40.60%\n    Epoch  4/10 | Train Loss: 1.7671 | Train Acc: 35.40% | Test Loss: 1.5551 | Test Acc: 41.74% | Best: 41.74%\n    Epoch  5/10 | Train Loss: 1.7022 | Train Acc: 38.18% | Test Loss: 1.4132 | Test Acc: 48.56% | Best: 48.56%\n    Epoch  6/10 | Train Loss: 1.6294 | Train Acc: 40.65% | Test Loss: 1.3990 | Test Acc: 49.12% | Best: 49.12%\n    Epoch  7/10 | Train Loss: 1.5693 | Train Acc: 43.26% | Test Loss: 1.3107 | Test Acc: 52.60% | Best: 52.60%\n    Epoch  8/10 | Train Loss: 1.5103 | Train Acc: 45.51% | Test Loss: 1.2480 | Test Acc: 55.58% | Best: 55.58%\n    Epoch  9/10 | Train Loss: 1.4654 | Train Acc: 47.45% | Test Loss: 1.2280 | Test Acc: 56.18% | Best: 56.18%\n    Epoch 10/10 | Train Loss: 1.4409 | Train Acc: 48.18% | Test Loss: 1.2204 | Test Acc: 56.04% | Best: 56.18%\n\n  Teacher Training Complete. Best Accuracy: 56.18%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000788 | KL: 205.7525 | Corr: 0.8367 | Best: 0.8367\n    Epoch  2/10 | MSE: 0.000299 | KL: 59.4175 | Corr: 0.9201 | Best: 0.9201\n    Epoch  3/10 | MSE: 0.000215 | KL: 40.6729 | Corr: 0.9414 | Best: 0.9414\n    Epoch  4/10 | MSE: 0.000165 | KL: 29.9790 | Corr: 0.9663 | Best: 0.9663\n    Epoch  5/10 | MSE: 0.000136 | KL: 24.1266 | Corr: 0.9730 | Best: 0.9730\n    Epoch  6/10 | MSE: 0.000121 | KL: 21.1625 | Corr: 0.9766 | Best: 0.9766\n    Epoch  7/10 | MSE: 0.000111 | KL: 19.3115 | Corr: 0.9794 | Best: 0.9794\n    Epoch  8/10 | MSE: 0.000104 | KL: 18.0137 | Corr: 0.9816 | Best: 0.9816\n    Epoch  9/10 | MSE: 0.000100 | KL: 17.1613 | Corr: 0.9827 | Best: 0.9827\n    Epoch 10/10 | MSE: 0.000097 | KL: 16.7044 | Corr: 0.9829 | Best: 0.9829\n\n  Distillation Complete. Best Correlation: 0.9829\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.001013 | KL: nan | Corr: 0.5428 | Best: 0.5428\n    Epoch  2/10 | MSE: 0.000745 | KL: nan | Corr: 0.6653 | Best: 0.6653\n    Epoch  3/10 | MSE: 0.000663 | KL: nan | Corr: 0.6998 | Best: 0.6998\n    Epoch  4/10 | MSE: 0.000653 | KL: nan | Corr: 0.7182 | Best: 0.7182\n    Epoch  5/10 | MSE: 0.000615 | KL: nan | Corr: 0.7389 | Best: 0.7389\n    Epoch  6/10 | MSE: 0.000685 | KL: nan | Corr: 0.7616 | Best: 0.7616\n    Epoch  7/10 | MSE: 0.000555 | KL: nan | Corr: 0.7818 | Best: 0.7818\n    Epoch  8/10 | MSE: 0.000546 | KL: nan | Corr: 0.7939 | Best: 0.7939\n    Epoch  9/10 | MSE: 0.000529 | KL: nan | Corr: 0.7966 | Best: 0.7966\n    Epoch 10/10 | MSE: 0.000526 | KL: nan | Corr: 0.7983 | Best: 0.7983\n\n  Distillation Complete. Best Correlation: 0.7983\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 33.1470 | Train: 31.77% | Test: 51.62% | Best: 51.62%\n    Epoch  2/10 | Loss: 29.7819 | Train: 43.15% | Test: 50.86% | Best: 51.62%\n    Epoch  3/10 | Loss: 25.2643 | Train: 42.96% | Test: 50.98% | Best: 51.62%\n    Epoch  4/10 | Loss: 18.4255 | Train: 44.62% | Test: 53.04% | Best: 53.04%\n    Epoch  5/10 | Loss: 10.0739 | Train: 47.19% | Test: 55.92% | Best: 55.92%\n    Epoch  6/10 | Loss: 1.4153 | Train: 49.48% | Test: 58.98% | Best: 58.98%\n    Epoch  7/10 | Loss: 1.3478 | Train: 51.47% | Test: 60.56% | Best: 60.56%\n    Epoch  8/10 | Loss: 1.2960 | Train: 53.76% | Test: 62.54% | Best: 62.54%\n    Epoch  9/10 | Loss: 1.2497 | Train: 55.30% | Test: 64.24% | Best: 64.24%\n    Epoch 10/10 | Loss: 1.2217 | Train: 56.72% | Test: 64.46% | Best: 64.46%\n\n  MSKA Training Complete. Best Accuracy: 64.46%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 22.02% | Test: 37.52% | Best: 37.52%\n    Epoch  2/10 | Loss: nan | Train: 35.35% | Test: 42.82% | Best: 42.82%\n    Epoch  3/10 | Loss: nan | Train: 38.18% | Test: 44.88% | Best: 44.88%\n    Epoch  4/10 | Loss: nan | Train: 39.71% | Test: 46.36% | Best: 46.36%\n    Epoch  5/10 | Loss: nan | Train: 41.24% | Test: 50.68% | Best: 50.68%\n    Epoch  6/10 | Loss: 1.5658 | Train: 43.60% | Test: 53.26% | Best: 53.26%\n    Epoch  7/10 | Loss: 1.5100 | Train: 45.26% | Test: 54.48% | Best: 54.48%\n    Epoch  8/10 | Loss: 1.4678 | Train: 47.09% | Test: 55.06% | Best: 55.06%\n    Epoch  9/10 | Loss: 1.4246 | Train: 48.74% | Test: 56.40% | Best: 56.40%\n    Epoch 10/10 | Loss: 1.3954 | Train: 49.78% | Test: 57.72% | Best: 57.72%\n\n  LNA Training Complete. Best Accuracy: 57.72%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.4020, TopK=0.2861\n  LNA Fidelity:  Corr=0.4690, TopK=0.3336\n\n  Analyzing MSKA Scale Weights...\n    Scale Ranks: [16, 64, 128]\n    Mean Weights: [0.04077765 0.27071208 0.6885103 ]\n    Std Weights:  [0.024239   0.02980464 0.04771864]\n    Attention maps saved to attention_maps_cifar_10/\n\n----------------------------------------------------------------------------------------\n  Seed 2/3: 123\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.81M\n    MSKA Student:          4.36M\n    LNA Student:           1.88M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1589 | Train Acc: 19.05% | Test Loss: 1.9718 | Test Acc: 26.50% | Best: 26.50%\n    Epoch  2/10 | Train Loss: 2.0025 | Train Acc: 25.23% | Test Loss: 1.8302 | Test Acc: 32.82% | Best: 32.82%\n    Epoch  3/10 | Train Loss: 1.8532 | Train Acc: 31.87% | Test Loss: 1.5631 | Test Acc: 43.74% | Best: 43.74%\n    Epoch  4/10 | Train Loss: 1.7599 | Train Acc: 36.02% | Test Loss: 1.5077 | Test Acc: 44.98% | Best: 44.98%\n    Epoch  5/10 | Train Loss: 1.6921 | Train Acc: 37.92% | Test Loss: 1.4487 | Test Acc: 47.86% | Best: 47.86%\n    Epoch  6/10 | Train Loss: 1.6248 | Train Acc: 41.08% | Test Loss: 1.3533 | Test Acc: 50.48% | Best: 50.48%\n    Epoch  7/10 | Train Loss: 1.5677 | Train Acc: 43.03% | Test Loss: 1.3032 | Test Acc: 52.36% | Best: 52.36%\n    Epoch  8/10 | Train Loss: 1.5106 | Train Acc: 45.54% | Test Loss: 1.2659 | Test Acc: 53.78% | Best: 53.78%\n    Epoch  9/10 | Train Loss: 1.4732 | Train Acc: 47.19% | Test Loss: 1.2290 | Test Acc: 55.44% | Best: 55.44%\n    Epoch 10/10 | Train Loss: 1.4422 | Train Acc: 47.96% | Test Loss: 1.2191 | Test Acc: 55.44% | Best: 55.44%\n\n  Teacher Training Complete. Best Accuracy: 55.44%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000757 | KL: 200.3574 | Corr: 0.8495 | Best: 0.8495\n    Epoch  2/10 | MSE: 0.000269 | KL: 50.4254 | Corr: 0.9279 | Best: 0.9279\n    Epoch  3/10 | MSE: 0.000175 | KL: 31.2099 | Corr: 0.9587 | Best: 0.9587\n    Epoch  4/10 | MSE: 0.000138 | KL: 23.9699 | Corr: 0.9684 | Best: 0.9684\n    Epoch  5/10 | MSE: 0.000117 | KL: 19.8781 | Corr: 0.9758 | Best: 0.9758\n    Epoch  6/10 | MSE: 0.000104 | KL: 17.5335 | Corr: 0.9796 | Best: 0.9796\n    Epoch  7/10 | MSE: 0.000094 | KL: 15.8258 | Corr: 0.9824 | Best: 0.9824\n    Epoch  8/10 | MSE: 0.000088 | KL: 14.8117 | Corr: 0.9840 | Best: 0.9840\n    Epoch  9/10 | MSE: 0.000085 | KL: 14.2360 | Corr: 0.9848 | Best: 0.9848\n    Epoch 10/10 | MSE: 0.000084 | KL: 14.0081 | Corr: 0.9850 | Best: 0.9850\n\n  Distillation Complete. Best Correlation: 0.9850\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.000949 | KL: nan | Corr: 0.6373 | Best: 0.6373\n    Epoch  2/10 | MSE: 0.000579 | KL: nan | Corr: 0.7270 | Best: 0.7270\n    Epoch  3/10 | MSE: 0.000529 | KL: nan | Corr: 0.7385 | Best: 0.7385\n    Epoch  4/10 | MSE: 0.000504 | KL: nan | Corr: 0.7586 | Best: 0.7586\n    Epoch  5/10 | MSE: 0.000479 | KL: nan | Corr: 0.7846 | Best: 0.7846\n    Epoch  6/10 | MSE: 0.000438 | KL: nan | Corr: 0.8108 | Best: 0.8108\n    Epoch  7/10 | MSE: 0.000412 | KL: nan | Corr: 0.8243 | Best: 0.8243\n    Epoch  8/10 | MSE: 0.000384 | KL: nan | Corr: 0.8360 | Best: 0.8360\n    Epoch  9/10 | MSE: 0.000368 | KL: nan | Corr: 0.8403 | Best: 0.8403\n    Epoch 10/10 | MSE: 0.000363 | KL: nan | Corr: 0.8414 | Best: 0.8414\n\n  Distillation Complete. Best Correlation: 0.8414\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 33.0883 | Train: 30.75% | Test: 48.14% | Best: 48.14%\n    Epoch  2/10 | Loss: 29.8767 | Train: 42.63% | Test: 51.24% | Best: 51.24%\n    Epoch  3/10 | Loss: 24.8866 | Train: 43.63% | Test: 51.80% | Best: 51.80%\n    Epoch  4/10 | Loss: 18.0938 | Train: 45.03% | Test: 55.34% | Best: 55.34%\n    Epoch  5/10 | Loss: 10.0223 | Train: 47.13% | Test: 55.58% | Best: 55.58%\n    Epoch  6/10 | Loss: 1.4240 | Train: 48.93% | Test: 59.64% | Best: 59.64%\n    Epoch  7/10 | Loss: 1.3626 | Train: 51.06% | Test: 61.26% | Best: 61.26%\n    Epoch  8/10 | Loss: 1.2928 | Train: 53.61% | Test: 62.90% | Best: 62.90%\n    Epoch  9/10 | Loss: 1.2550 | Train: 55.10% | Test: 64.00% | Best: 64.00%\n    Epoch 10/10 | Loss: 1.2244 | Train: 56.41% | Test: 64.26% | Best: 64.26%\n\n  MSKA Training Complete. Best Accuracy: 64.26%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 23.92% | Test: 35.58% | Best: 35.58%\n    Epoch  2/10 | Loss: nan | Train: 37.04% | Test: 46.24% | Best: 46.24%\n    Epoch  3/10 | Loss: nan | Train: 40.01% | Test: 46.28% | Best: 46.28%\n    Epoch  4/10 | Loss: nan | Train: 40.85% | Test: 51.10% | Best: 51.10%\n    Epoch  5/10 | Loss: nan | Train: 42.46% | Test: 50.66% | Best: 51.10%\n    Epoch  6/10 | Loss: 1.5192 | Train: 45.07% | Test: 54.52% | Best: 54.52%\n    Epoch  7/10 | Loss: 1.4732 | Train: 47.10% | Test: 56.76% | Best: 56.76%\n    Epoch  8/10 | Loss: 1.4213 | Train: 49.02% | Test: 57.10% | Best: 57.10%\n    Epoch  9/10 | Loss: 1.3795 | Train: 50.54% | Test: 58.18% | Best: 58.18%\n    Epoch 10/10 | Loss: 1.3528 | Train: 51.44% | Test: 58.52% | Best: 58.52%\n\n  LNA Training Complete. Best Accuracy: 58.52%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.4040, TopK=0.3172\n  LNA Fidelity:  Corr=0.5236, TopK=0.4026\n\n----------------------------------------------------------------------------------------\n  Seed 3/3: 456\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.81M\n    MSKA Student:          4.36M\n    LNA Student:           1.88M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1720 | Train Acc: 18.21% | Test Loss: 1.8982 | Test Acc: 30.24% | Best: 30.24%\n    Epoch  2/10 | Train Loss: 2.0028 | Train Acc: 25.21% | Test Loss: 1.7601 | Test Acc: 33.12% | Best: 33.12%\n    Epoch  3/10 | Train Loss: 1.8702 | Train Acc: 31.18% | Test Loss: 1.5902 | Test Acc: 40.92% | Best: 40.92%\n    Epoch  4/10 | Train Loss: 1.7679 | Train Acc: 35.48% | Test Loss: 1.6001 | Test Acc: 40.30% | Best: 40.92%\n    Epoch  5/10 | Train Loss: 1.6903 | Train Acc: 38.47% | Test Loss: 1.4660 | Test Acc: 46.92% | Best: 46.92%\n    Epoch  6/10 | Train Loss: 1.6379 | Train Acc: 40.45% | Test Loss: 1.3793 | Test Acc: 49.56% | Best: 49.56%\n    Epoch  7/10 | Train Loss: 1.5530 | Train Acc: 44.08% | Test Loss: 1.3326 | Test Acc: 50.34% | Best: 50.34%\n    Epoch  8/10 | Train Loss: 1.5179 | Train Acc: 45.45% | Test Loss: 1.2613 | Test Acc: 54.26% | Best: 54.26%\n    Epoch  9/10 | Train Loss: 1.4657 | Train Acc: 47.02% | Test Loss: 1.2326 | Test Acc: 55.24% | Best: 55.24%\n    Epoch 10/10 | Train Loss: 1.4507 | Train Acc: 48.05% | Test Loss: 1.2293 | Test Acc: 55.42% | Best: 55.42%\n\n  Teacher Training Complete. Best Accuracy: 55.42%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000778 | KL: 207.3940 | Corr: 0.8160 | Best: 0.8160\n    Epoch  2/10 | MSE: 0.000335 | KL: 67.1400 | Corr: 0.9092 | Best: 0.9092\n    Epoch  3/10 | MSE: 0.000213 | KL: 39.6726 | Corr: 0.9452 | Best: 0.9452\n    Epoch  4/10 | MSE: 0.000169 | KL: 30.5369 | Corr: 0.9596 | Best: 0.9596\n    Epoch  5/10 | MSE: 0.000143 | KL: 25.3682 | Corr: 0.9666 | Best: 0.9666\n    Epoch  6/10 | MSE: 0.000127 | KL: 22.1352 | Corr: 0.9731 | Best: 0.9731\n    Epoch  7/10 | MSE: 0.000118 | KL: 20.5141 | Corr: 0.9717 | Best: 0.9731\n    Epoch  8/10 | MSE: 0.000113 | KL: 19.5866 | Corr: 0.9777 | Best: 0.9777\n    Epoch  9/10 | MSE: 0.000109 | KL: 18.6916 | Corr: 0.9782 | Best: 0.9782\n    Epoch 10/10 | MSE: 0.000107 | KL: 18.2946 | Corr: 0.9789 | Best: 0.9789\n\n  Distillation Complete. Best Correlation: 0.9789\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.000981 | KL: nan | Corr: 0.5102 | Best: 0.5102\n    Epoch  2/10 | MSE: 0.000724 | KL: nan | Corr: 0.6517 | Best: 0.6517\n    Epoch  3/10 | MSE: 0.000667 | KL: nan | Corr: 0.6516 | Best: 0.6517\n    Epoch  4/10 | MSE: 0.000641 | KL: nan | Corr: 0.6979 | Best: 0.6979\n    Epoch  5/10 | MSE: 0.000589 | KL: nan | Corr: 0.6751 | Best: 0.6979\n    Epoch  6/10 | MSE: 0.000565 | KL: nan | Corr: 0.7429 | Best: 0.7429\n    Epoch  7/10 | MSE: 0.000509 | KL: nan | Corr: 0.7652 | Best: 0.7652\n    Epoch  8/10 | MSE: 0.000482 | KL: nan | Corr: 0.7835 | Best: 0.7835\n    Epoch  9/10 | MSE: 0.000466 | KL: nan | Corr: 0.7921 | Best: 0.7921\n    Epoch 10/10 | MSE: 0.000459 | KL: nan | Corr: 0.7947 | Best: 0.7947\n\n  Distillation Complete. Best Correlation: 0.7947\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 33.3952 | Train: 33.48% | Test: 49.78% | Best: 49.78%\n    Epoch  2/10 | Loss: 29.8185 | Train: 42.68% | Test: 52.72% | Best: 52.72%\n    Epoch  3/10 | Loss: 24.9296 | Train: 43.86% | Test: 51.28% | Best: 52.72%\n    Epoch  4/10 | Loss: 18.2273 | Train: 44.84% | Test: 54.74% | Best: 54.74%\n    Epoch  5/10 | Loss: 9.9983 | Train: 47.21% | Test: 56.98% | Best: 56.98%\n    Epoch  6/10 | Loss: 1.4115 | Train: 49.47% | Test: 57.92% | Best: 57.92%\n    Epoch  7/10 | Loss: 1.3426 | Train: 51.69% | Test: 59.84% | Best: 59.84%\n    Epoch  8/10 | Loss: 1.3012 | Train: 53.52% | Test: 61.56% | Best: 61.56%\n    Epoch  9/10 | Loss: 1.2405 | Train: 55.53% | Test: 63.56% | Best: 63.56%\n    Epoch 10/10 | Loss: 1.2146 | Train: 56.61% | Test: 63.96% | Best: 63.96%\n\n  MSKA Training Complete. Best Accuracy: 63.96%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 22.52% | Test: 40.16% | Best: 40.16%\n    Epoch  2/10 | Loss: nan | Train: 36.45% | Test: 43.12% | Best: 43.12%\n    Epoch  3/10 | Loss: nan | Train: 39.07% | Test: 46.30% | Best: 46.30%\n    Epoch  4/10 | Loss: nan | Train: 40.17% | Test: 49.10% | Best: 49.10%\n    Epoch  5/10 | Loss: nan | Train: 42.47% | Test: 51.40% | Best: 51.40%\n    Epoch  6/10 | Loss: 1.5555 | Train: 43.58% | Test: 53.66% | Best: 53.66%\n    Epoch  7/10 | Loss: 1.5004 | Train: 46.07% | Test: 55.40% | Best: 55.40%\n    Epoch  8/10 | Loss: 1.4510 | Train: 47.90% | Test: 56.04% | Best: 56.04%\n    Epoch  9/10 | Loss: 1.4108 | Train: 49.77% | Test: 57.16% | Best: 57.16%\n    Epoch 10/10 | Loss: 1.3884 | Train: 50.20% | Test: 57.12% | Best: 57.16%\n\n  LNA Training Complete. Best Accuracy: 57.16%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.3773, TopK=0.2843\n  LNA Fidelity:  Corr=0.4331, TopK=0.3331\n\n----------------------------------------------------------------------------------------\n  Statistical Analysis\n----------------------------------------------------------------------------------------\n\n  Model Accuracies:\n    Teacher   : 55.68% ± 0.43% | CI: (54.60%, 56.76%)\n    MSKA      : 64.23% ± 0.25% | CI: (63.60%, 64.85%)\n    LNA       : 57.80% ± 0.68% | CI: (56.10%, 59.50%)\n\n  Pairwise Comparisons:\n    Teacher_vs_MSKA     : p=0.000333, d=-24.1286, sig=True\n    Teacher_vs_LNA      : p=0.048271, d=-3.7051, sig=True\n    MSKA_vs_LNA         : p=0.002849, d=12.4780, sig=True\n\n----------------------------------------------------------------------------------------\n  Attention Fidelity Summary\n----------------------------------------------------------------------------------------\n  MSKA: Corr=0.3945, TopK=0.2959, MSE=0.000210\n  LNA:  Corr=0.4752, TopK=0.3564, MSE=0.000233\n\n----------------------------------------------------------------------------------------\n  Theoretical Complexity\n----------------------------------------------------------------------------------------\n\n  Standard Attention:\n    Complexity: O(n²d) = O(65² × 192) = O(811200)\n    Total FLOPs: 124,750,080\n\n  MSKA (Multi-Scale Kernel):\n    Complexity: O(n² × S) where S=3 scales, r_max=128\n    Total FLOPs: 273,696,876\n    Reduction: -119.4%\n    Speedup: 0.46x\n\n  LNA (Learned Nyström):\n    Complexity: O(n × m²) = O(65 × 16²) = O(16640)\n    Total FLOPs: 118,082,340\n    Reduction: 5.3%\n    Speedup: 1.06x\n\n========================================================================================\n                                 EXPERIMENT: CIFAR-100                                  \n========================================================================================\n\n  Loading CIFAR-100 (50% of data)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169M/169M [02:13<00:00, 1.27MB/s] \n","output_type":"stream"},{"name":"stdout","text":"  Classes: 100, Image size: 32x32\n  Train batches: 97, Test batches: 20\n  Number of tokens: 65\n\n----------------------------------------------------------------------------------------\n  Seed 1/3: 42\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.82M\n    MSKA Student:          4.38M\n    LNA Student:           1.90M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 4.5178 | Train Acc: 2.70% | Test Loss: 4.2977 | Test Acc: 5.18% | Best: 5.18%\n    Epoch  8/10 | Train Loss: 3.4224 | Train Acc: 18.50% | Test Loss: 3.0684 | Test Acc: 25.02% | Best: 25.02%\n    Epoch  9/10 | Train Loss: 3.3516 | Train Acc: 19.94% | Test Loss: 3.0304 | Test Acc: 26.28% | Best: 26.28%\n    Epoch 10/10 | Train Loss: 3.3181 | Train Acc: 20.61% | Test Loss: 3.0124 | Test Acc: 26.82% | Best: 26.82%\n\n  Teacher Training Complete. Best Accuracy: 26.82%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000801 | KL: 217.1115 | Corr: 0.8353 | Best: 0.8353\n    Epoch  2/10 | MSE: 0.000337 | KL: 70.9348 | Corr: 0.9071 | Best: 0.9071\n    Epoch  3/10 | MSE: 0.000235 | KL: 46.4378 | Corr: 0.9427 | Best: 0.9427\n    Epoch  4/10 | MSE: 0.000184 | KL: 35.1402 | Corr: 0.9563 | Best: 0.9563\n    Epoch  5/10 | MSE: 0.000159 | KL: 29.8131 | Corr: 0.9661 | Best: 0.9661\n    Epoch  6/10 | MSE: 0.000143 | KL: 26.5862 | Corr: 0.9700 | Best: 0.9700\n    Epoch  7/10 | MSE: 0.000132 | KL: 24.2113 | Corr: 0.9736 | Best: 0.9736\n    Epoch  8/10 | MSE: 0.000125 | KL: 22.9263 | Corr: 0.9754 | Best: 0.9754\n    Epoch  9/10 | MSE: 0.000121 | KL: 21.9260 | Corr: 0.9765 | Best: 0.9765\n    Epoch 10/10 | MSE: 0.000119 | KL: 21.5716 | Corr: 0.9769 | Best: 0.9769\n\n  Distillation Complete. Best Correlation: 0.9769\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.001035 | KL: nan | Corr: 0.5170 | Best: 0.5170\n    Epoch  2/10 | MSE: 0.000767 | KL: nan | Corr: 0.6525 | Best: 0.6525\n    Epoch  3/10 | MSE: 0.000667 | KL: nan | Corr: 0.7011 | Best: 0.7011\n    Epoch  4/10 | MSE: 0.000646 | KL: nan | Corr: 0.6929 | Best: 0.7011\n    Epoch  5/10 | MSE: 0.000624 | KL: nan | Corr: 0.7297 | Best: 0.7297\n    Epoch  6/10 | MSE: 0.000579 | KL: nan | Corr: 0.7537 | Best: 0.7537\n    Epoch  7/10 | MSE: 0.000537 | KL: nan | Corr: 0.7665 | Best: 0.7665\n    Epoch  8/10 | MSE: 0.000512 | KL: nan | Corr: 0.7807 | Best: 0.7807\n    Epoch  9/10 | MSE: 0.000498 | KL: nan | Corr: 0.7851 | Best: 0.7851\n    Epoch 10/10 | MSE: 0.000488 | KL: nan | Corr: 0.7870 | Best: 0.7870\n\n  Distillation Complete. Best Correlation: 0.7870\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 36.0930 | Train: 5.88% | Test: 13.86% | Best: 13.86%\n    Epoch  2/10 | Loss: 32.8121 | Train: 12.84% | Test: 18.22% | Best: 18.22%\n    Epoch  3/10 | Loss: 27.6937 | Train: 15.43% | Test: 21.04% | Best: 21.04%\n    Epoch  4/10 | Loss: 20.6684 | Train: 16.92% | Test: 22.54% | Best: 22.54%\n    Epoch  5/10 | Loss: 12.1487 | Train: 18.77% | Test: 25.28% | Best: 25.28%\n    Epoch  6/10 | Loss: 3.2424 | Train: 21.14% | Test: 28.94% | Best: 28.94%\n    Epoch  7/10 | Loss: 3.1302 | Train: 23.77% | Test: 31.64% | Best: 31.64%\n    Epoch  8/10 | Loss: 3.0341 | Train: 25.13% | Test: 33.26% | Best: 33.26%\n    Epoch  9/10 | Loss: 2.9500 | Train: 27.48% | Test: 34.06% | Best: 34.06%\n    Epoch 10/10 | Loss: 2.9085 | Train: 28.15% | Test: 33.96% | Best: 34.06%\n\n  MSKA Training Complete. Best Accuracy: 34.06%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 2.83% | Test: 8.44% | Best: 8.44%\n    Epoch  2/10 | Loss: nan | Train: 8.51% | Test: 13.74% | Best: 13.74%\n    Epoch  3/10 | Loss: nan | Train: 11.85% | Test: 16.42% | Best: 16.42%\n    Epoch  4/10 | Loss: nan | Train: 13.63% | Test: 18.74% | Best: 18.74%\n    Epoch  5/10 | Loss: nan | Train: 15.79% | Test: 22.26% | Best: 22.26%\n    Epoch  6/10 | Loss: 3.4968 | Train: 17.29% | Test: 23.40% | Best: 23.40%\n    Epoch  7/10 | Loss: 3.3868 | Train: 19.02% | Test: 25.48% | Best: 25.48%\n    Epoch  8/10 | Loss: 3.2960 | Train: 20.59% | Test: 27.66% | Best: 27.66%\n    Epoch  9/10 | Loss: 3.2262 | Train: 22.34% | Test: 27.80% | Best: 27.80%\n    Epoch 10/10 | Loss: 3.1998 | Train: 22.58% | Test: 28.36% | Best: 28.36%\n\n  LNA Training Complete. Best Accuracy: 28.36%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.3994, TopK=0.2789\n  LNA Fidelity:  Corr=0.3565, TopK=0.2734\n\n  Analyzing MSKA Scale Weights...\n    Scale Ranks: [16, 64, 128]\n    Mean Weights: [0.04515469 0.28855607 0.6662893 ]\n    Std Weights:  [0.02458422 0.03444459 0.05233166]\n    Attention maps saved to attention_maps_cifar_100/\n\n----------------------------------------------------------------------------------------\n  Seed 2/3: 123\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.82M\n    MSKA Student:          4.38M\n    LNA Student:           1.90M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 4.5162 | Train Acc: 2.48% | Test Loss: 4.2772 | Test Acc: 4.98% | Best: 4.98%\n    Epoch  2/10 | Train Loss: 4.2879 | Train Acc: 5.51% | Test Loss: 3.9786 | Test Acc: 9.10% | Best: 9.10%\n    Epoch  3/10 | Train Loss: 4.0867 | Train Acc: 8.26% | Test Loss: 3.7588 | Test Acc: 12.82% | Best: 12.82%\n    Epoch  4/10 | Train Loss: 3.9454 | Train Acc: 9.86% | Test Loss: 3.6537 | Test Acc: 14.54% | Best: 14.54%\n    Epoch  5/10 | Train Loss: 3.7824 | Train Acc: 12.75% | Test Loss: 3.4032 | Test Acc: 18.94% | Best: 18.94%\n    Epoch  6/10 | Train Loss: 3.6518 | Train Acc: 15.04% | Test Loss: 3.3038 | Test Acc: 19.50% | Best: 19.50%\n    Epoch  7/10 | Train Loss: 3.5492 | Train Acc: 16.39% | Test Loss: 3.1805 | Test Acc: 22.76% | Best: 22.76%\n    Epoch  8/10 | Train Loss: 3.4470 | Train Acc: 18.09% | Test Loss: 3.0827 | Test Acc: 24.70% | Best: 24.70%\n    Epoch  9/10 | Train Loss: 3.3749 | Train Acc: 19.74% | Test Loss: 3.0367 | Test Acc: 25.26% | Best: 25.26%\n    Epoch 10/10 | Train Loss: 3.3329 | Train Acc: 20.10% | Test Loss: 3.0322 | Test Acc: 25.84% | Best: 25.84%\n\n  Teacher Training Complete. Best Accuracy: 25.84%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000902 | KL: 242.8485 | Corr: 0.7914 | Best: 0.7914\n    Epoch  2/10 | MSE: 0.000405 | KL: 86.2752 | Corr: 0.8923 | Best: 0.8923\n    Epoch  3/10 | MSE: 0.000272 | KL: 54.1328 | Corr: 0.9356 | Best: 0.9356\n    Epoch  4/10 | MSE: 0.000216 | KL: 41.2577 | Corr: 0.9510 | Best: 0.9510\n    Epoch  5/10 | MSE: 0.000184 | KL: 34.3202 | Corr: 0.9624 | Best: 0.9624\n    Epoch  6/10 | MSE: 0.000165 | KL: 30.2959 | Corr: 0.9668 | Best: 0.9668\n    Epoch  7/10 | MSE: 0.000151 | KL: 27.4498 | Corr: 0.9716 | Best: 0.9716\n    Epoch  8/10 | MSE: 0.000142 | KL: 25.8258 | Corr: 0.9732 | Best: 0.9732\n    Epoch  9/10 | MSE: 0.000138 | KL: 24.8972 | Corr: 0.9748 | Best: 0.9748\n    Epoch 10/10 | MSE: 0.000136 | KL: 24.3931 | Corr: 0.9751 | Best: 0.9751\n\n  Distillation Complete. Best Correlation: 0.9751\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.001126 | KL: nan | Corr: 0.5335 | Best: 0.5335\n    Epoch  2/10 | MSE: 0.000822 | KL: nan | Corr: 0.6387 | Best: 0.6387\n    Epoch  3/10 | MSE: 0.000771 | KL: nan | Corr: 0.6416 | Best: 0.6416\n    Epoch  4/10 | MSE: 0.000719 | KL: nan | Corr: 0.7051 | Best: 0.7051\n    Epoch  5/10 | MSE: 0.000689 | KL: nan | Corr: 0.7043 | Best: 0.7051\n    Epoch  6/10 | MSE: 0.000700 | KL: nan | Corr: 0.7099 | Best: 0.7099\n    Epoch  7/10 | MSE: 0.000644 | KL: nan | Corr: 0.7398 | Best: 0.7398\n    Epoch  8/10 | MSE: 0.000603 | KL: nan | Corr: 0.7489 | Best: 0.7489\n    Epoch  9/10 | MSE: 0.000588 | KL: nan | Corr: 0.7517 | Best: 0.7517\n    Epoch 10/10 | MSE: 0.000580 | KL: nan | Corr: 0.7566 | Best: 0.7566\n\n  Distillation Complete. Best Correlation: 0.7566\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 36.5020 | Train: 5.83% | Test: 13.66% | Best: 13.66%\n    Epoch  2/10 | Loss: 33.2500 | Train: 12.82% | Test: 18.26% | Best: 18.26%\n    Epoch  3/10 | Loss: 27.7711 | Train: 15.21% | Test: 21.02% | Best: 21.02%\n    Epoch  4/10 | Loss: 20.8757 | Train: 16.70% | Test: 22.24% | Best: 22.24%\n    Epoch  5/10 | Loss: 12.1492 | Train: 18.60% | Test: 25.94% | Best: 25.94%\n    Epoch  6/10 | Loss: 3.2535 | Train: 21.36% | Test: 28.24% | Best: 28.24%\n    Epoch  7/10 | Loss: 3.1447 | Train: 23.15% | Test: 30.00% | Best: 30.00%\n    Epoch  8/10 | Loss: 3.0414 | Train: 25.47% | Test: 32.08% | Best: 32.08%\n    Epoch  9/10 | Loss: 2.9655 | Train: 27.10% | Test: 33.14% | Best: 33.14%\n    Epoch 10/10 | Loss: 2.9098 | Train: 27.97% | Test: 33.36% | Best: 33.36%\n\n  MSKA Training Complete. Best Accuracy: 33.36%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 3.36% | Test: 9.02% | Best: 9.02%\n    Epoch  2/10 | Loss: nan | Train: 8.41% | Test: 13.92% | Best: 13.92%\n    Epoch  3/10 | Loss: nan | Train: 11.06% | Test: 16.40% | Best: 16.40%\n    Epoch  4/10 | Loss: nan | Train: 13.27% | Test: 17.74% | Best: 17.74%\n    Epoch  5/10 | Loss: nan | Train: 14.89% | Test: 20.20% | Best: 20.20%\n    Epoch  6/10 | Loss: 3.5261 | Train: 16.63% | Test: 22.86% | Best: 22.86%\n    Epoch  7/10 | Loss: 3.4034 | Train: 19.00% | Test: 24.88% | Best: 24.88%\n    Epoch  8/10 | Loss: 3.3183 | Train: 20.38% | Test: 27.52% | Best: 27.52%\n    Epoch  9/10 | Loss: 3.2514 | Train: 21.83% | Test: 27.94% | Best: 27.94%\n    Epoch 10/10 | Loss: 3.2135 | Train: 22.12% | Test: 27.88% | Best: 27.94%\n\n  LNA Training Complete. Best Accuracy: 27.94%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.4103, TopK=0.3001\n  LNA Fidelity:  Corr=0.3312, TopK=0.2686\n\n----------------------------------------------------------------------------------------\n  Seed 3/3: 456\n----------------------------------------------------------------------------------------\n\n  Model Parameters:\n    Teacher (Standard):    1.82M\n    MSKA Student:          4.38M\n    LNA Student:           1.90M\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 4.5253 | Train Acc: 2.65% | Test Loss: 4.2998 | Test Acc: 4.84% | Best: 4.84%\n    Epoch  2/10 | Train Loss: 4.3089 | Train Acc: 4.97% | Test Loss: 3.9948 | Test Acc: 9.52% | Best: 9.52%\n    Epoch  3/10 | Train Loss: 4.1081 | Train Acc: 7.48% | Test Loss: 3.7693 | Test Acc: 12.82% | Best: 12.82%\n    Epoch  4/10 | Train Loss: 3.9188 | Train Acc: 10.10% | Test Loss: 3.5600 | Test Acc: 15.32% | Best: 15.32%\n    Epoch  5/10 | Train Loss: 3.7727 | Train Acc: 12.73% | Test Loss: 3.4492 | Test Acc: 17.38% | Best: 17.38%\n    Epoch  6/10 | Train Loss: 3.6328 | Train Acc: 14.79% | Test Loss: 3.2830 | Test Acc: 20.62% | Best: 20.62%\n    Epoch  7/10 | Train Loss: 3.5148 | Train Acc: 17.38% | Test Loss: 3.1377 | Test Acc: 23.96% | Best: 23.96%\n    Epoch  8/10 | Train Loss: 3.4194 | Train Acc: 18.69% | Test Loss: 3.0710 | Test Acc: 24.98% | Best: 24.98%\n    Epoch  9/10 | Train Loss: 3.3544 | Train Acc: 19.64% | Test Loss: 3.0189 | Test Acc: 26.70% | Best: 26.70%\n    Epoch 10/10 | Train Loss: 3.3159 | Train Acc: 20.92% | Test Loss: 3.0162 | Test Acc: 26.52% | Best: 26.70%\n\n  Teacher Training Complete. Best Accuracy: 26.70%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000852 | KL: 228.5782 | Corr: 0.8112 | Best: 0.8112\n    Epoch  2/10 | MSE: 0.000362 | KL: 75.8632 | Corr: 0.9069 | Best: 0.9069\n    Epoch  3/10 | MSE: 0.000248 | KL: 48.9548 | Corr: 0.9389 | Best: 0.9389\n    Epoch  4/10 | MSE: 0.000198 | KL: 37.6658 | Corr: 0.9549 | Best: 0.9549\n    Epoch  5/10 | MSE: 0.000168 | KL: 31.2785 | Corr: 0.9642 | Best: 0.9642\n    Epoch  6/10 | MSE: 0.000150 | KL: 27.5008 | Corr: 0.9691 | Best: 0.9691\n    Epoch  7/10 | MSE: 0.000138 | KL: 25.0412 | Corr: 0.9724 | Best: 0.9724\n    Epoch  8/10 | MSE: 0.000130 | KL: 23.4861 | Corr: 0.9749 | Best: 0.9749\n    Epoch  9/10 | MSE: 0.000124 | KL: 22.3427 | Corr: 0.9765 | Best: 0.9765\n    Epoch 10/10 | MSE: 0.000122 | KL: 21.9069 | Corr: 0.9768 | Best: 0.9768\n\n  Distillation Complete. Best Correlation: 0.9768\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (LNA)\n----------------------------------------------------------------------------------------\n  Trainable parameters: 1,879,110\n    Epoch  1/10 | MSE: 0.001080 | KL: nan | Corr: 0.4824 | Best: 0.4824\n    Epoch  2/10 | MSE: 0.000833 | KL: nan | Corr: 0.5829 | Best: 0.5829\n    Epoch  3/10 | MSE: 0.000720 | KL: nan | Corr: 0.6829 | Best: 0.6829\n    Epoch  4/10 | MSE: 0.000664 | KL: nan | Corr: 0.7199 | Best: 0.7199\n    Epoch  5/10 | MSE: 0.000612 | KL: nan | Corr: 0.7443 | Best: 0.7443\n    Epoch  6/10 | MSE: 0.000587 | KL: nan | Corr: 0.7701 | Best: 0.7701\n    Epoch  7/10 | MSE: 0.000548 | KL: nan | Corr: 0.7911 | Best: 0.7911\n    Epoch  8/10 | MSE: 0.000517 | KL: nan | Corr: 0.8008 | Best: 0.8008\n    Epoch  9/10 | MSE: 0.000505 | KL: nan | Corr: 0.8052 | Best: 0.8052\n    Epoch 10/10 | MSE: 0.000499 | KL: nan | Corr: 0.8065 | Best: 0.8065\n\n  Distillation Complete. Best Correlation: 0.8065\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 35.9051 | Train: 5.55% | Test: 14.36% | Best: 14.36%\n    Epoch  2/10 | Loss: 33.1908 | Train: 12.91% | Test: 19.62% | Best: 19.62%\n    Epoch  3/10 | Loss: 27.7630 | Train: 15.24% | Test: 20.14% | Best: 20.14%\n    Epoch  4/10 | Loss: 20.5749 | Train: 17.24% | Test: 23.32% | Best: 23.32%\n    Epoch  5/10 | Loss: 12.0925 | Train: 19.25% | Test: 26.86% | Best: 26.86%\n    Epoch  6/10 | Loss: 3.2433 | Train: 21.74% | Test: 27.84% | Best: 27.84%\n    Epoch  7/10 | Loss: 3.1543 | Train: 23.20% | Test: 31.56% | Best: 31.56%\n    Epoch  8/10 | Loss: 3.0262 | Train: 26.05% | Test: 32.90% | Best: 32.90%\n    Epoch  9/10 | Loss: 2.9498 | Train: 27.09% | Test: 33.90% | Best: 33.90%\n    Epoch 10/10 | Loss: 2.9129 | Train: 28.19% | Test: 33.80% | Best: 33.90%\n\n  MSKA Training Complete. Best Accuracy: 33.90%\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (LNA)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: nan | Train: 3.24% | Test: 7.46% | Best: 7.46%\n    Epoch  2/10 | Loss: nan | Train: 8.60% | Test: 13.02% | Best: 13.02%\n    Epoch  3/10 | Loss: nan | Train: 11.59% | Test: 16.64% | Best: 16.64%\n    Epoch  4/10 | Loss: nan | Train: 13.24% | Test: 18.16% | Best: 18.16%\n    Epoch  5/10 | Loss: nan | Train: 15.19% | Test: 20.96% | Best: 20.96%\n    Epoch  6/10 | Loss: 3.4949 | Train: 17.12% | Test: 23.72% | Best: 23.72%\n    Epoch  7/10 | Loss: 3.3848 | Train: 19.40% | Test: 26.34% | Best: 26.34%\n    Epoch  8/10 | Loss: 3.2896 | Train: 20.79% | Test: 26.98% | Best: 26.98%\n    Epoch  9/10 | Loss: 3.2181 | Train: 22.37% | Test: 28.88% | Best: 28.88%\n    Epoch 10/10 | Loss: 3.1904 | Train: 22.87% | Test: 29.22% | Best: 29.22%\n\n  LNA Training Complete. Best Accuracy: 29.22%\n\n  Computing Attention Fidelity...\n\n  MSKA Fidelity: Corr=0.4279, TopK=0.3140\n  LNA Fidelity:  Corr=0.4319, TopK=0.3297\n\n----------------------------------------------------------------------------------------\n  Statistical Analysis\n----------------------------------------------------------------------------------------\n\n  Model Accuracies:\n    Teacher   : 26.45% ± 0.53% | CI: (25.13%, 27.78%)\n    MSKA      : 33.77% ± 0.37% | CI: (32.86%, 34.68%)\n    LNA       : 28.51% ± 0.65% | CI: (26.89%, 30.13%)\n\n  Pairwise Comparisons:\n    Teacher_vs_MSKA     : p=0.000189, d=-15.9685, sig=True\n    Teacher_vs_LNA      : p=0.018581, d=-3.4427, sig=True\n    MSKA_vs_LNA         : p=0.003321, d=9.9507, sig=True\n\n----------------------------------------------------------------------------------------\n  Attention Fidelity Summary\n----------------------------------------------------------------------------------------\n  MSKA: Corr=0.4125, TopK=0.2976, MSE=0.000211\n  LNA:  Corr=0.3732, TopK=0.2906, MSE=0.000291\n\n----------------------------------------------------------------------------------------\n  Theoretical Complexity\n----------------------------------------------------------------------------------------\n\n  Standard Attention:\n    Complexity: O(n²d) = O(65² × 192) = O(811200)\n    Total FLOPs: 124,750,080\n\n  MSKA (Multi-Scale Kernel):\n    Complexity: O(n² × S) where S=3 scales, r_max=128\n    Total FLOPs: 273,696,876\n    Reduction: -119.4%\n    Speedup: 0.46x\n\n  LNA (Learned Nyström):\n    Complexity: O(n × m²) = O(65 × 16²) = O(16640)\n    Total FLOPs: 118,082,340\n    Reduction: 5.3%\n    Speedup: 1.06x\n\n========================================================================================\n                                    ABLATION STUDIES                                    \n========================================================================================\n\n----------------------------------------------------------------------------------------\n  Ablation: MSKA Scale Configurations\n----------------------------------------------------------------------------------------\n\n  Testing scales = [16, 32]\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1651 | Train Acc: 18.84% | Test Loss: 1.9878 | Test Acc: 26.56% | Best: 26.56%\n    Epoch  2/10 | Train Loss: 2.0123 | Train Acc: 24.61% | Test Loss: 1.8209 | Test Acc: 33.28% | Best: 33.28%\n    Epoch  3/10 | Train Loss: 1.8585 | Train Acc: 32.22% | Test Loss: 1.6860 | Test Acc: 40.50% | Best: 40.50%\n    Epoch  4/10 | Train Loss: 1.7628 | Train Acc: 35.93% | Test Loss: 1.5470 | Test Acc: 42.52% | Best: 42.52%\n    Epoch  5/10 | Train Loss: 1.6978 | Train Acc: 38.66% | Test Loss: 1.4397 | Test Acc: 48.00% | Best: 48.00%\n    Epoch  6/10 | Train Loss: 1.6310 | Train Acc: 41.18% | Test Loss: 1.3701 | Test Acc: 49.58% | Best: 49.58%\n    Epoch  7/10 | Train Loss: 1.5644 | Train Acc: 43.73% | Test Loss: 1.2853 | Test Acc: 53.36% | Best: 53.36%\n    Epoch  8/10 | Train Loss: 1.5104 | Train Acc: 45.57% | Test Loss: 1.2416 | Test Acc: 55.48% | Best: 55.48%\n    Epoch  9/10 | Train Loss: 1.4648 | Train Acc: 46.80% | Test Loss: 1.2059 | Test Acc: 57.12% | Best: 57.12%\n    Epoch 10/10 | Train Loss: 1.4385 | Train Acc: 48.49% | Test Loss: 1.2070 | Test Acc: 56.94% | Best: 57.12%\n\n  Teacher Training Complete. Best Accuracy: 57.12%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA-[16, 32])\n----------------------------------------------------------------------------------------\n  Trainable parameters: 2,108,748\n    Epoch  1/10 | MSE: 0.001006 | KL: 255.1389 | Corr: 0.7430 | Best: 0.7430\n    Epoch  2/10 | MSE: 0.000494 | KL: 86.2214 | Corr: 0.8631 | Best: 0.8631\n    Epoch  3/10 | MSE: 0.000352 | KL: 57.8420 | Corr: 0.9065 | Best: 0.9065\n    Epoch  4/10 | MSE: 0.000284 | KL: 46.3688 | Corr: 0.9311 | Best: 0.9311\n    Epoch  5/10 | MSE: 0.000234 | KL: 36.9187 | Corr: 0.9436 | Best: 0.9436\n    Epoch  6/10 | MSE: 0.000213 | KL: 33.5653 | Corr: 0.9312 | Best: 0.9436\n    Epoch  7/10 | MSE: 0.000205 | KL: 32.1786 | Corr: 0.9509 | Best: 0.9509\n    Epoch  8/10 | MSE: 0.000194 | KL: 29.7619 | Corr: 0.9534 | Best: 0.9534\n    Epoch  9/10 | MSE: 0.000185 | KL: 28.1353 | Corr: 0.9562 | Best: 0.9562\n    Epoch 10/10 | MSE: 0.000181 | KL: 27.4594 | Corr: 0.9567 | Best: 0.9567\n\n  Distillation Complete. Best Correlation: 0.9567\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA-[16, 32])\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 1.9077 | Train: 30.50% | Test: 47.24% | Best: 47.24%\n    Epoch  2/10 | Loss: 1.5970 | Train: 42.60% | Test: 50.92% | Best: 50.92%\n    Epoch  3/10 | Loss: 1.5544 | Train: 44.12% | Test: 52.10% | Best: 52.10%\n    Epoch  4/10 | Loss: 1.5255 | Train: 45.44% | Test: 51.70% | Best: 52.10%\n    Epoch  5/10 | Loss: 1.4631 | Train: 47.32% | Test: 56.16% | Best: 56.16%\n    Epoch  6/10 | Loss: 1.4069 | Train: 49.31% | Test: 59.30% | Best: 59.30%\n    Epoch  7/10 | Loss: 1.3560 | Train: 50.91% | Test: 60.96% | Best: 60.96%\n    Epoch  8/10 | Loss: 1.2950 | Train: 53.62% | Test: 62.42% | Best: 62.42%\n    Epoch  9/10 | Loss: 1.2628 | Train: 54.71% | Test: 63.16% | Best: 63.16%\n    Epoch 10/10 | Loss: 1.2420 | Train: 55.84% | Test: 63.40% | Best: 63.40%\n\n  MSKA-[16, 32] Training Complete. Best Accuracy: 63.40%\n  Scales [16, 32]: Acc=63.40%, Corr=0.3782\n\n  Testing scales = [16, 64, 128]\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1619 | Train Acc: 18.71% | Test Loss: 1.9405 | Test Acc: 26.14% | Best: 26.14%\n    Epoch  2/10 | Train Loss: 2.0075 | Train Acc: 24.96% | Test Loss: 1.7493 | Test Acc: 36.26% | Best: 36.26%\n    Epoch  3/10 | Train Loss: 1.8703 | Train Acc: 31.40% | Test Loss: 1.6069 | Test Acc: 41.52% | Best: 41.52%\n    Epoch  4/10 | Train Loss: 1.7652 | Train Acc: 35.68% | Test Loss: 1.5896 | Test Acc: 40.86% | Best: 41.52%\n    Epoch  5/10 | Train Loss: 1.6913 | Train Acc: 38.64% | Test Loss: 1.5027 | Test Acc: 46.40% | Best: 46.40%\n    Epoch  6/10 | Train Loss: 1.6154 | Train Acc: 41.92% | Test Loss: 1.3277 | Test Acc: 51.66% | Best: 51.66%\n    Epoch  7/10 | Train Loss: 1.5768 | Train Acc: 42.92% | Test Loss: 1.2994 | Test Acc: 52.74% | Best: 52.74%\n    Epoch  8/10 | Train Loss: 1.5162 | Train Acc: 45.20% | Test Loss: 1.2603 | Test Acc: 54.48% | Best: 54.48%\n    Epoch  9/10 | Train Loss: 1.4696 | Train Acc: 47.12% | Test Loss: 1.2187 | Test Acc: 55.44% | Best: 55.44%\n    Epoch 10/10 | Train Loss: 1.4443 | Train Acc: 48.30% | Test Loss: 1.2148 | Test Acc: 56.10% | Best: 56.10%\n\n  Teacher Training Complete. Best Accuracy: 56.10%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA-[16, 64, 128])\n----------------------------------------------------------------------------------------\n  Trainable parameters: 4,355,538\n    Epoch  1/10 | MSE: 0.000887 | KL: 212.8465 | Corr: 0.8305 | Best: 0.8305\n    Epoch  2/10 | MSE: 0.000351 | KL: 63.1137 | Corr: 0.9112 | Best: 0.9112\n    Epoch  3/10 | MSE: 0.000238 | KL: 40.2250 | Corr: 0.9477 | Best: 0.9477\n    Epoch  4/10 | MSE: 0.000182 | KL: 29.7431 | Corr: 0.9640 | Best: 0.9640\n    Epoch  5/10 | MSE: 0.000157 | KL: 25.1933 | Corr: 0.9704 | Best: 0.9704\n    Epoch  6/10 | MSE: 0.000135 | KL: 21.2512 | Corr: 0.9764 | Best: 0.9764\n    Epoch  7/10 | MSE: 0.000126 | KL: 19.7627 | Corr: 0.9783 | Best: 0.9783\n    Epoch  8/10 | MSE: 0.000120 | KL: 18.6875 | Corr: 0.9799 | Best: 0.9799\n    Epoch  9/10 | MSE: 0.000115 | KL: 17.8036 | Corr: 0.9814 | Best: 0.9814\n    Epoch 10/10 | MSE: 0.000112 | KL: 17.4029 | Corr: 0.9818 | Best: 0.9818\n\n  Distillation Complete. Best Correlation: 0.9818\n\n----------------------------------------------------------------------------------------\n  Phase 3: Classification Training (MSKA-[16, 64, 128])\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Loss: 1.9192 | Train: 30.32% | Test: 49.70% | Best: 49.70%\n    Epoch  2/10 | Loss: 1.5869 | Train: 42.69% | Test: 53.30% | Best: 53.30%\n    Epoch  3/10 | Loss: 1.5467 | Train: 44.67% | Test: 53.68% | Best: 53.68%\n    Epoch  4/10 | Loss: 1.5257 | Train: 44.88% | Test: 55.80% | Best: 55.80%\n    Epoch  5/10 | Loss: 1.4836 | Train: 46.67% | Test: 57.60% | Best: 57.60%\n    Epoch  6/10 | Loss: 1.4038 | Train: 49.32% | Test: 59.46% | Best: 59.46%\n    Epoch  7/10 | Loss: 1.3549 | Train: 51.27% | Test: 59.38% | Best: 59.46%\n    Epoch  8/10 | Loss: 1.2995 | Train: 53.28% | Test: 63.28% | Best: 63.28%\n    Epoch  9/10 | Loss: 1.2513 | Train: 54.99% | Test: 64.78% | Best: 64.78%\n    Epoch 10/10 | Loss: 1.2292 | Train: 55.73% | Test: 64.72% | Best: 64.78%\n\n  MSKA-[16, 64, 128] Training Complete. Best Accuracy: 64.78%\n  Scales [16, 64, 128]: Acc=64.78%, Corr=0.3850\n\n  Testing scales = [32, 64, 128, 256]\n\n----------------------------------------------------------------------------------------\n  Phase 1: Training Teacher (Standard ViT)\n----------------------------------------------------------------------------------------\n    Epoch  1/10 | Train Loss: 2.1680 | Train Acc: 18.31% | Test Loss: 1.9340 | Test Acc: 25.94% | Best: 25.94%\n    Epoch  2/10 | Train Loss: 2.0079 | Train Acc: 24.79% | Test Loss: 1.7701 | Test Acc: 35.26% | Best: 35.26%\n    Epoch  3/10 | Train Loss: 1.8571 | Train Acc: 31.77% | Test Loss: 1.5954 | Test Acc: 40.90% | Best: 40.90%\n    Epoch  4/10 | Train Loss: 1.7591 | Train Acc: 35.93% | Test Loss: 1.5135 | Test Acc: 44.34% | Best: 44.34%\n    Epoch  5/10 | Train Loss: 1.6973 | Train Acc: 38.20% | Test Loss: 1.4545 | Test Acc: 47.92% | Best: 47.92%\n    Epoch  6/10 | Train Loss: 1.6243 | Train Acc: 41.21% | Test Loss: 1.3632 | Test Acc: 50.32% | Best: 50.32%\n    Epoch  7/10 | Train Loss: 1.5687 | Train Acc: 43.38% | Test Loss: 1.3022 | Test Acc: 53.40% | Best: 53.40%\n    Epoch  8/10 | Train Loss: 1.5176 | Train Acc: 45.21% | Test Loss: 1.2477 | Test Acc: 55.42% | Best: 55.42%\n    Epoch  9/10 | Train Loss: 1.4657 | Train Acc: 47.51% | Test Loss: 1.2196 | Test Acc: 56.24% | Best: 56.24%\n    Epoch 10/10 | Train Loss: 1.4384 | Train Acc: 48.60% | Test Loss: 1.2182 | Test Acc: 56.40% | Best: 56.40%\n\n  Teacher Training Complete. Best Accuracy: 56.40%\n\n----------------------------------------------------------------------------------------\n  Phase 2: Distillation (MSKA-[32, 64, 128, 256])\n----------------------------------------------------------------------------------------\n  Trainable parameters: 8,174,808\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;31m# Ablation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m     \u001b[0mablation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ablation_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ablation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablation_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36mrun_ablation_study\u001b[0;34m(train_loader, test_loader, num_classes, img_size, cfg)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0mteacher_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m         \u001b[0mtrain_distillation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"MSKA-{scales}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m         \u001b[0mstudent_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_student_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"MSKA-{scales}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36mtrain_distillation\u001b[0;34m(student, teacher, train_loader, test_loader, cfg, student_name)\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_attns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mstudent_attns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_attention_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_attention_distillation_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_attns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36mget_all_attention_maps\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mattn_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0mattn_maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1397242489.py\u001b[0m in \u001b[0;36mpredict_attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_scale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_attns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mcombined_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_attn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 3675 has 15.87 GiB memory in use. Of the allocated memory 15.07 GiB is allocated by PyTorch, and 504.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 3675 has 15.87 GiB memory in use. Of the allocated memory 15.07 GiB is allocated by PyTorch, and 504.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":1}]}